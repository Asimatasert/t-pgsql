#!/bin/bash
#
# t-pgsql - PostgreSQL Database Sync & Clone Tool
# https://github.com/Asimatasert/t-pgsql
#
# Usage: t-pgsql <command> [options]
#

set -e

# ==============================================================================
# VERSION & PATHS
# ==============================================================================
VERSION="3.7.0"
SCRIPT_NAME="t-pgsql"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# ==============================================================================
# COLORS
# ==============================================================================
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
MAGENTA='\033[0;35m'
BOLD='\033[1m'
NC='\033[0m'

# ==============================================================================
# DEFAULT VALUES
# ==============================================================================

# Command
COMMAND=""

# Connection
FROM_CONNECTION=""
TO_CONNECTIONS=()

# Password
FROM_PASSWORD=""
TO_PASSWORD=""
PASSWORD=""
FROM_PASSWORD_FILE=""
TO_PASSWORD_FILES=()
PASSWORD_FILE=""

# Config
CONFIG_FILE=""

# Filtering
EXCLUDE_TABLES=""
EXCLUDE_SCHEMAS=""
EXCLUDE_DATA=""
ONLY_TABLES=""
ONLY_SCHEMAS=""

# Compression
COMPRESS="gzip"
COMPRESS_LEVEL=6
PG_COMPRESS_LEVEL=6

# Storage
OUTPUT_DIR="${SCRIPT_DIR}/../data/dumps"
KEEP=-1
FROM_KEEP=1  # 0=delete, N=keep last N, -1=keep all (default: 1)
SKIP_IF_RECENT=""  # Skip if dump exists within timeframe (e.g., 24h, 12h, today)

# Retention (GFS)
RETENTION=false
RETENTION_DAILY=7
RETENTION_WEEKLY=4
RETENTION_MONTHLY=12
RETENTION_YEARLY=3

# Health Check
HEALTH_CHECK=true
HEALTH_CHECK_AFTER=false
HEALTH_CHECK_FAIL=false

# Notifications
NOTIFY=()
NOTIFY_ON_ERROR=false

# Masking
MASK=false
MASK_RULES=""
MASK_TABLES=""

# Streaming
STREAM=false
STREAM_BUFFER=64

# Sudo
SUDO=false

# Batch
PARALLEL=1
CONTINUE_ON_ERROR=false
ONLY_JOBS=""
EXCLUDE_JOBS=""
NOTIFY_SUMMARY=false
SAVE_JOB=""
BATCH_JOB=""
JOBS_FILE="${SCRIPT_DIR}/jobs.yaml"
JOBS_ACTION=""
JOBS_TARGET=""

# General
VERBOSE=false
QUIET=false
DRY_RUN=false
YES=false
FORCE=false
LOG_FILE=""
LOG_LEVEL="info"

# Restore
FILE=""

# Fetch (existing dump from source)
FROM_FILE=""

# Metadata & Timing
META_ENABLED=true
META_START_TIME=""
META_START_EPOCH=""
META_STATUS="unknown"
META_EXIT_CODE=0

# ==============================================================================
# PARSED CONNECTION VARIABLES
# ==============================================================================
FROM_TYPE=""
FROM_SSH_USER=""
FROM_SSH_HOST=""
FROM_SSH_PORT="22"
FROM_DB_USER="postgres"
FROM_DB_HOST="localhost"
FROM_DB_PORT="5432"
FROM_DB_PASSWORD=""
FROM_DATABASE=""

TO_TYPE=""
TO_SSH_USER=""
TO_SSH_HOST=""
TO_SSH_PORT="22"
TO_DB_USER="postgres"
TO_DB_HOST="localhost"
TO_DB_PORT="5432"
TO_DB_PASSWORD=""
TO_DATABASE=""

# ==============================================================================
# SOURCE MODULES
# ==============================================================================
source_module() {
    local module="$1"
    local module_path="${SCRIPT_DIR}/modules/${module}.sh"
    if [ -f "$module_path" ]; then
        source "$module_path"
    fi
}

# ==============================================================================
# LOGGING
# ==============================================================================
log_info() {
    [ "$QUIET" != true ] && echo -e "${GREEN}[INFO]${NC} $1"
    log_to_file "INFO" "$1"
}

log_warn() {
    [ "$QUIET" != true ] && echo -e "${YELLOW}[WARN]${NC} $1"
    log_to_file "WARN" "$1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" >&2
    log_to_file "ERROR" "$1"
}

log_success() {
    [ "$QUIET" != true ] && echo -e "${GREEN}[OK]${NC} $1"
    log_to_file "OK" "$1"
}

log_debug() {
    [ "$VERBOSE" = true ] && echo -e "${CYAN}[DEBUG]${NC} $1"
    log_to_file "DEBUG" "$1"
}

log_to_file() {
    if [ -n "$LOG_FILE" ]; then
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] [$1] $2" >> "$LOG_FILE"
    fi
}

# ==============================================================================
# NOTIFICATIONS
# ==============================================================================
send_notification() {
    local status="$1"      # success, failed, error
    local message="$2"     # Main message
    local details="$3"     # Additional details (optional)

    [ ${#NOTIFY[@]} -eq 0 ] && return 0
    [ "$QUIET" = true ] && return 0

    # Only notify on error if --notify-on-error is set
    if [ "$NOTIFY_ON_ERROR" = true ] && [ "$status" = "success" ]; then
        return 0
    fi

    local emoji="‚úÖ"
    [ "$status" = "failed" ] || [ "$status" = "error" ] && emoji="‚ùå"
    [ "$status" = "warning" ] && emoji="‚ö†Ô∏è"

    local hostname=$(hostname)
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')

    for channel in "${NOTIFY[@]}"; do
        case "$channel" in
            telegram:*) notify_telegram "$channel" "$emoji" "$message" "$details" "$timestamp" "$hostname" ;;
            webhook:*)  notify_webhook "$channel" "$status" "$message" "$details" "$timestamp" ;;
            slack:*)    notify_slack "$channel" "$emoji" "$message" "$details" "$timestamp" "$hostname" ;;
            email:*)    notify_email "$channel" "$status" "$message" "$details" ;;
            *)          log_warn "Unknown notification channel: $channel" ;;
        esac
    done
}

notify_telegram() {
    local channel="$1"
    local emoji="$2"
    local message="$3"
    local details="$4"
    local timestamp="$5"
    local hostname="$6"

    # Parse telegram:BOT_ID:BOT_SECRET:CHAT_ID or telegram:BOT_ID:BOT_SECRET:CHAT_ID:THREAD_ID
    # Token format: BOT_ID:BOT_SECRET (contains colon)
    local token=$(echo "$channel" | cut -d: -f2-3)
    local chat_id=$(echo "$channel" | cut -d: -f4)
    local thread_id=$(echo "$channel" | cut -d: -f5)

    if [ -z "$token" ] || [ -z "$chat_id" ]; then
        log_warn "Invalid Telegram config. Use: telegram:TOKEN:CHAT_ID[:THREAD_ID]"
        return 1
    fi

    local text="${emoji} ${message}

${details}
üïê ${timestamp}"

    local curl_args=(-s -X POST "https://api.telegram.org/bot${token}/sendMessage"
        -d "chat_id=${chat_id}"
        -d "text=${text}"
        -d "parse_mode=Markdown"
        -d "disable_web_page_preview=true")

    # Add thread_id for forum topics
    [ -n "$thread_id" ] && curl_args+=(-d "message_thread_id=${thread_id}")

    local response=$(curl "${curl_args[@]}" 2>&1)

    if echo "$response" | grep -q '"ok":true'; then
        log_debug "Telegram notification sent"
    else
        log_warn "Telegram notification failed: $response"
    fi
}

notify_webhook() {
    local channel="$1"
    local status="$2"
    local message="$3"
    local details="$4"
    local timestamp="$5"

    local url="${channel#webhook:}"

    local payload=$(cat <<EOF
{
    "status": "$status",
    "message": "$message",
    "details": "$details",
    "timestamp": "$timestamp",
    "tool": "t-pgsql",
    "version": "$VERSION"
}
EOF
)

    local response=$(curl -s -X POST "$url" \
        -H "Content-Type: application/json" \
        -d "$payload" 2>&1)

    log_debug "Webhook response: $response"
}

notify_slack() {
    local channel="$1"
    local emoji="$2"
    local message="$3"
    local details="$4"
    local timestamp="$5"
    local hostname="$6"

    local webhook_url="${channel#slack:}"

    local color="good"
    [[ "$emoji" == "‚ùå" ]] && color="danger"
    [[ "$emoji" == "‚ö†Ô∏è" ]] && color="warning"

    local payload=$(cat <<EOF
{
    "attachments": [{
        "color": "$color",
        "title": "$emoji t-pgsql: $message",
        "text": "$details",
        "footer": "Host: $hostname | $timestamp"
    }]
}
EOF
)

    curl -s -X POST "$webhook_url" \
        -H "Content-Type: application/json" \
        -d "$payload" >/dev/null 2>&1

    log_debug "Slack notification sent"
}

notify_email() {
    local channel="$1"
    local status="$2"
    local message="$3"
    local details="$4"

    local email="${channel#email:}"

    if command -v mail &>/dev/null; then
        echo -e "Status: $status\n\n$message\n\nDetails:\n$details" | \
            mail -s "[t-pgsql] $status: $message" "$email"
        log_debug "Email notification sent to $email"
    else
        log_warn "mail command not found, skipping email notification"
    fi
}

# Build notification details for operations
build_notify_details() {
    local operation="$1"
    local status="$2"
    local elapsed="$3"
    local size="${4:-}"

    local details=""

    # Source info
    if [ -n "$FROM_DATABASE" ]; then
        local src_host="${FROM_SSH_HOST:-${FROM_DB_HOST}}"
        details+="üì§ \`${src_host}\`/\`${FROM_DATABASE}\`\n"
    fi

    # Target info
    if [ -n "$TO_DATABASE" ]; then
        local tgt_host="${TO_SSH_HOST:-${TO_DB_HOST}}"
        details+="üì• \`${tgt_host}\`/\`${TO_DATABASE}\`\n"
    fi

    # Size and duration on same line if both exist
    local metrics=""
    [ -n "$size" ] && metrics+="üì¶ ${size}"
    [ -n "$elapsed" ] && [ -n "$metrics" ] && metrics+="  ‚Ä¢  "
    [ -n "$elapsed" ] && metrics+="‚è± ${elapsed}"
    [ -n "$metrics" ] && details+="${metrics}\n"

    # Exclusions
    local exclusions=""
    [ -n "$EXCLUDE_TABLES" ] && exclusions+="tables: ${EXCLUDE_TABLES}, "
    [ -n "$EXCLUDE_DATA" ] && exclusions+="data: ${EXCLUDE_DATA}, "
    [ -n "$EXCLUDE_SCHEMAS" ] && exclusions+="schemas: ${EXCLUDE_SCHEMAS}, "
    if [ -n "$exclusions" ]; then
        exclusions="${exclusions%, }"  # Remove trailing comma
        details+="üö´ ${exclusions}\n"
    fi

    # Retention info
    local retention=""
    [ "$FROM_KEEP" -eq 0 ] && retention+="source: none, "
    [ "$FROM_KEEP" -gt 0 ] && retention+="source: ${FROM_KEEP}, "
    [ "$KEEP" -eq 0 ] && retention+="local: none"
    [ "$KEEP" -gt 0 ] && retention+="local: ${KEEP}"
    if [ -n "$retention" ] && [ "$retention" != "source: 1, local: -1" ]; then
        retention="${retention%, }"  # Remove trailing comma
        details+="üíæ keep: ${retention}\n"
    fi

    echo -e "$details"
}

# ==============================================================================
# METADATA
# ==============================================================================
meta_start() {
    META_START_TIME=$(date '+%Y-%m-%d %H:%M:%S')
    META_START_EPOCH=$(date +%s)
    META_STATUS="running"
}

meta_write() {
    local dump_file="$1"
    local status="${2:-success}"
    local exit_code="${3:-0}"

    [ "$META_ENABLED" != true ] && return 0
    [ -z "$dump_file" ] && return 0
    [ ! -f "$dump_file" ] && return 0

    local dump_dir=$(dirname "$dump_file")
    local dump_name=$(basename "$dump_file")
    local base_name="${dump_name%.dump}"
    local meta_file="${dump_dir}/metadata.yaml"
    local tar_file="${dump_dir}/${base_name}.tar.gz"

    local end_time=$(date '+%Y-%m-%d %H:%M:%S')
    local end_epoch=$(date +%s)
    local elapsed_sec=$((end_epoch - META_START_EPOCH))
    local elapsed=$(format_elapsed $elapsed_sec)
    local dump_size=$(ls -lh "$dump_file" 2>/dev/null | awk '{print $5}')

    # Create metadata.yaml
    cat > "$meta_file" << EOF
# t-pgsql metadata
# Generated: ${end_time}

timing:
  started_at: "${META_START_TIME}"
  finished_at: "${end_time}"
  elapsed: "${elapsed}"
  elapsed_seconds: ${elapsed_sec}

source:
  type: ${FROM_TYPE}
  host: ${FROM_SSH_HOST:-${FROM_DB_HOST}}
  port: ${FROM_DB_PORT}
  database: ${FROM_DATABASE}
  user: ${FROM_DB_USER}

file:
  name: ${dump_name}
  size: "${dump_size:-unknown}"
  compression: ${COMPRESS}
  compress_level: ${PG_COMPRESS_LEVEL}

operation:
  command: ${COMMAND}
  status: ${status}
  exit_code: ${exit_code}

environment:
  script_version: "${VERSION}"
  executed_by: $(whoami)
  executed_on: $(hostname)
  working_dir: $(pwd)
EOF

    # Filter bilgisi varsa ekle
    if [ -n "$EXCLUDE_TABLES" ] || [ -n "$EXCLUDE_SCHEMAS" ]; then
        cat >> "$meta_file" << EOF

filter:
  exclude_tables: "${EXCLUDE_TABLES}"
  exclude_schemas: "${EXCLUDE_SCHEMAS}"
  exclude_data: "${EXCLUDE_DATA}"
  only_tables: "${ONLY_TABLES}"
  only_schemas: "${ONLY_SCHEMAS}"
EOF
    fi

    # Create tar.gz archive
    log_debug "Creating archive: $tar_file"
    local tar_files=("$dump_name")
    [ -f "$meta_file" ] && tar_files+=("metadata.yaml")
    (cd "$dump_dir" && tar -czf "$(basename "$tar_file")" "${tar_files[@]}" 2>/dev/null)

    if [ $? -eq 0 ]; then
        # Remove original files
        rm -f "$dump_file" "$meta_file"
        log_debug "Archive created: $tar_file"

        # Return the tar file path (for use by caller)
        echo "TAR_FILE=$tar_file"
    else
        log_warn "Failed to create archive, keeping separate files"
    fi
}

meta_update_target() {
    local tar_file="$1"
    local restore_status="${2:-success}"
    local restore_elapsed="${3:-0}"

    [ "$META_ENABLED" != true ] && return 0
    [ -z "$tar_file" ] && return 0
    [ ! -f "$tar_file" ] && return 0

    local temp_dir=$(mktemp -d)

    # Extract everything from tar
    tar -xzf "$tar_file" -C "$temp_dir" 2>/dev/null || { rm -rf "$temp_dir"; return 0; }

    # Find the dump file
    local dump_name=$(ls "$temp_dir"/*.dump 2>/dev/null | head -1 | xargs basename 2>/dev/null)
    [ -z "$dump_name" ] && { rm -rf "$temp_dir"; return 0; }

    # Create metadata.yaml if it doesn't exist
    if [ ! -f "$temp_dir/metadata.yaml" ]; then
        cat > "$temp_dir/metadata.yaml" << EOF
# t-pgsql metadata (generated during restore)
timing:
  restored_at: "$(date '+%Y-%m-%d %H:%M:%S')"
EOF
    fi

    # Append target and restore info to metadata
    cat >> "$temp_dir/metadata.yaml" << EOF

target:
  type: ${TO_TYPE}
  host: ${TO_SSH_HOST:-${TO_DB_HOST}}
  port: ${TO_DB_PORT}
  database: ${TO_DATABASE}
  user: ${TO_DB_USER}

restore:
  status: ${restore_status}
  elapsed: "$(format_elapsed $restore_elapsed)"
  total_elapsed: "$(format_elapsed $(($(date +%s) - META_START_EPOCH)))"
EOF

    # Recreate tar with updated metadata
    local tar_args=("$dump_name")
    [ -f "$temp_dir/metadata.yaml" ] && tar_args+=("metadata.yaml")
    (cd "$temp_dir" && tar -czf "$tar_file" "${tar_args[@]}" 2>/dev/null)

    rm -rf "$temp_dir"
    log_debug "Metadata updated with target info"
}

# Extract dump from tar archive, returns path to extracted dump
extract_dump() {
    local file="$1"
    local extract_dir="${2:-$(mktemp -d)}"

    if [[ "$file" == *.tar.gz ]]; then
        local dump_name=$(tar -tzf "$file" 2>/dev/null | grep -E '\.dump$' | head -1)
        if [ -n "$dump_name" ]; then
            tar -xzf "$file" -C "$extract_dir" "$dump_name" 2>/dev/null
            echo "${extract_dir}/${dump_name}"
        fi
    elif [[ "$file" == *.dump ]]; then
        echo "$file"
    fi
}

# Show metadata from tar archive
show_meta() {
    local file="$1"

    if [[ "$file" == *.tar.gz ]]; then
        tar -xzf "$file" -O metadata.yaml 2>/dev/null
    elif [[ -f "${file}.meta" ]]; then
        cat "${file}.meta"
    fi
}

format_elapsed() {
    local sec=$1
    local min=$((sec / 60))
    local hrs=$((min / 60))
    sec=$((sec % 60))
    min=$((min % 60))

    if [ $hrs -gt 0 ]; then
        printf "%dh %dm %ds" $hrs $min $sec
    elif [ $min -gt 0 ]; then
        printf "%dm %ds" $min $sec
    else
        printf "%ds" $sec
    fi
}

# Check if recent dump exists (skip_if_recent feature)
# Returns 0 if should skip, 1 if should proceed
check_skip_recent() {
    [ -z "$SKIP_IF_RECENT" ] && return 1  # No skip configured, proceed

    local dump_base_name="${DUMP_NAME:-$FROM_DATABASE}"
    local latest=$(ls -t "$OUTPUT_DIR"/${dump_base_name}_*.tar.gz 2>/dev/null | head -1)
    [ -z "$latest" ] && latest=$(ls -t "$OUTPUT_DIR"/${dump_base_name}_*.dump 2>/dev/null | head -1)
    [ -z "$latest" ] && return 1  # No existing dump, proceed

    # Get file modification time
    local file_epoch=$(stat -f %m "$latest" 2>/dev/null || stat -c %Y "$latest" 2>/dev/null)
    [ -z "$file_epoch" ] && return 1

    local now_epoch=$(date +%s)
    local diff_sec=$((now_epoch - file_epoch))

    # Parse timeframe
    local skip_sec=0
    case "$SKIP_IF_RECENT" in
        today)
            # Check if file was created today
            local file_date=$(date -r "$file_epoch" +%Y%m%d 2>/dev/null || date -d "@$file_epoch" +%Y%m%d 2>/dev/null)
            local today_date=$(date +%Y%m%d)
            if [ "$file_date" = "$today_date" ]; then
                log_info "Skipping: dump already exists today ($(basename "$latest"))"
                return 0
            fi
            return 1
            ;;
        *h)
            skip_sec=$(( ${SKIP_IF_RECENT%h} * 3600 ))
            ;;
        *m)
            skip_sec=$(( ${SKIP_IF_RECENT%m} * 60 ))
            ;;
        *d)
            skip_sec=$(( ${SKIP_IF_RECENT%d} * 86400 ))
            ;;
        *)
            # Assume hours if no suffix
            skip_sec=$(( SKIP_IF_RECENT * 3600 ))
            ;;
    esac

    if [ $diff_sec -lt $skip_sec ]; then
        local age=$(format_elapsed $diff_sec)
        log_info "Skipping: recent dump exists ($age ago, threshold: $SKIP_IF_RECENT)"
        return 0
    fi

    return 1
}

# ==============================================================================
# HELP
# ==============================================================================
show_help() {
    cat << 'EOF'
t-pgsql - PostgreSQL Database Sync & Clone Tool

USAGE:
    t-pgsql <command> [options]

COMMANDS:
    dump        Create database backup
    restore     Restore backup to database
    clone       Dump + Restore (full sync)
    fetch       Fetch existing dump from remote (no new dump)
    batch       Run multiple jobs from config
    jobs        Manage saved jobs
                  jobs [list] [--yaml <file>]  List all jobs
                  jobs show <name> [--yaml <file>]  Show job details
                  jobs remove <name> [--yaml <file>] Remove a job
    list        List dump files
    meta        Show metadata from archive
    clean       Clean old dumps
    version     Show version

CONNECTION:
    --from <connection>           Source connection
    --to <connection>             Target connection (repeatable)

    Format:
      Local:  [user@]host[:port]/database
      Remote: ssh://[ssh_user@]host[:port]/[db_user@]host[:port]/database

    Examples:
      localhost/mydb
      postgres@localhost:5432/mydb
      ssh://ubuntu@192.168.1.100/mydb
      ssh://ubuntu@192.168.1.100:22/postgres@localhost:5432/mydb

PASSWORD:
    --password <pass>             Password for both connections
    --from-password <pass>        Source password
    --to-password <pass>          Target password
    --password-file <file>        Read password from file
    --from-password-file <file>   Source password file
    --to-password-file <file>     Target password file (repeatable for multiple targets)
    --config <file>               Config file with credentials

    Env vars: T_PGSQL_PASSWORD, T_PGSQL_FROM_PASSWORD, T_PGSQL_TO_PASSWORD

FILTER:
    --exclude-table <t1,t2>       Exclude tables
    --exclude-schema <s1,s2>      Exclude schemas
    --exclude-data <t1,t2>        Exclude data only (keep structure, supports schema.* wildcard)
    --only-table <t1,t2>          Include only these tables
    --only-schema <s1,s2>         Include only these schemas

COMPRESSION:
    --compress <type>             gzip|zstd|xz|bzip2|none (default: gzip)
    --compress-level <1-9>        Compression level (default: 6)
    --pg-compress-level <0-9>     pg_dump compression (default: 6)

STORAGE:
    --output <dir>                Output directory
    --dump-name <name>            Custom dump filename (without timestamp)
    --keep <N>                    Keep last N local dumps (-1=all, 0=none)
    --from-keep <N>               Keep N dumps on source (default: 1, -1=all, 0=delete)
    --skip-if-recent <time>       Skip if dump exists within timeframe
                                  Examples: 24h, 12h, 1d, today
    --from-file [pattern]         Fetch existing dump (for fetch command)
                                  No value = latest dump for database
                                  Pattern: filename or glob (e.g., mydb_*.dump)

RETENTION (GFS):
    --retention                   Enable GFS retention
    --retention-daily <N>         Daily backups (default: 7)
    --retention-weekly <N>        Weekly backups (default: 4)
    --retention-monthly <N>       Monthly backups (default: 12)
    --retention-yearly <N>        Yearly backups (default: 3)

HEALTH CHECK:
    --health-check                Check before operation (default)
    --health-check-after          Check after operation
    --no-health-check             Disable checks
    --health-check-fail           Abort on check failure

NOTIFY:
    --notify <channel>            Notification channel (repeatable)
                                  telegram|telegram:TOKEN:CHAT
                                  webhook:URL|email:ADDR|slack:URL
    --notify-on-error             Only notify on error
    --quiet                       No notifications

MASKING:
    --mask                        Enable data masking
    --mask-rules <file>           Masking rules JSON
    --mask-tables <t1,t2>         Tables to mask

STREAMING:
    --stream                      Stream without temp files
    --stream-buffer <MB>          Buffer size (default: 64)

BATCH:
    --yaml <name>                 Use <name>.yaml instead of jobs.yaml
                                  Example: --yaml prod (uses prod.yaml)
    --parallel <N>                Parallel jobs (default: 1)
    --continue-on-error           Don't stop on error
    --only <jobs>                 Run only these jobs
    --exclude <jobs>              Skip these jobs
    --notify-summary              Summary notification

RESTORE:
    --file <path>                 Dump file to restore

GENERAL:
    --log <file>                  Log file
    --log-level <level>           debug|info|warn|error
    -v, --verbose                 Verbose output
    -q, --quiet                   Minimal output
    -y, --yes                     Skip confirmations
    --dry-run                     Show actions without executing
    --no-meta                     Don't write .meta files
    -h, --help                    Show help
    --version                     Show version

EXAMPLES:
    # Remote to local
    t-pgsql clone \
      --from ssh://ubuntu@192.168.1.100/prod_db \
      --to localhost/dev_db \
      --from-password-file ~/.secrets/prod.pass \
      --to-password-file ~/.secrets/local.pass

    # Local to remote (push)
    t-pgsql clone \
      --from localhost/mydb \
      --to ssh://ubuntu@192.168.1.100/backup_db \
      --password-file ~/.secrets/db.pass

    # Dump with retention
    t-pgsql dump \
      --from ssh://ubuntu@192.168.1.100/prod \
      --from-password-file ~/.secrets/prod.pass \
      --compress zstd \
      --retention

    # Restore
    t-pgsql restore \
      --file ./dumps/prod_20250130.dump \
      --to localhost/test_db \
      --to-password-file ~/.secrets/local.pass

    # Batch
    t-pgsql batch --config jobs.yaml --parallel 2

    # Fetch latest dump for database (auto-find)
    t-pgsql fetch --from ssh://ubuntu@192.168.1.100/prod --from-file

    # Fetch with specific pattern
    t-pgsql fetch \
      --from ssh://ubuntu@192.168.1.100/prod \
      --from-file "prod_20250130*.dump"

More info: https://github.com/Asimatasert/t-pgsql
EOF
}

show_version() {
    echo "${SCRIPT_NAME} v${VERSION}"
}

# ==============================================================================
# CONNECTION PARSER
# ==============================================================================
parse_connection() {
    local conn="$1"
    local prefix="$2"

    log_debug "Parsing: $conn -> $prefix"

    if [[ "$conn" == ssh://* ]]; then
        parse_ssh_connection "$conn" "$prefix"
    else
        parse_local_connection "$conn" "$prefix"
    fi
}

parse_local_connection() {
    local conn="$1"
    local prefix="$2"

    local user="postgres"
    local host="localhost"
    local port="5432"
    local db=""

    # Format: user@host[:port]/[db_user@]database
    # Examples:
    #   asimatasert@localhost/test           -> user=asimatasert, db=test
    #   asimatasert@localhost/postgres@test  -> user=postgres, db=test
    #   asimatasert@localhost:5432/test      -> with port
    #   postgres@test                        -> simple format, localhost

    if [[ "$conn" == */* ]]; then
        # Has slash: user@host/database or user@host:port/database
        local db_part="${conn##*/}"
        conn="${conn%/*}"

        # Check if db_part has user@ prefix (db_user@database)
        if [[ "$db_part" == *@* ]]; then
            user="${db_part%%@*}"
            db="${db_part#*@}"
        else
            db="$db_part"
            # user comes from left side of @
            if [[ "$conn" == *@* ]]; then
                user="${conn%%@*}"
            fi
        fi

        # Parse host part: user@host or user@host:port
        if [[ "$conn" == *@* ]]; then
            conn="${conn#*@}"
        fi

        # host:port or just host
        if [[ "$conn" == *:* ]]; then
            host="${conn%%:*}"
            port="${conn##*:}"
        else
            host="$conn"
        fi
    elif [[ "$conn" == *@* ]]; then
        # Simple format: user@database (localhost assumed)
        user="${conn%%@*}"
        db="${conn#*@}"
        host="localhost"
    else
        # Just database name
        db="$conn"
        host="localhost"
    fi

    [ -z "$db" ] && { log_error "Missing database: $1"; return 1; }

    eval "${prefix}_TYPE='local'"
    eval "${prefix}_DB_USER='$user'"
    eval "${prefix}_DB_HOST='$host'"
    eval "${prefix}_DB_PORT='$port'"
    eval "${prefix}_DATABASE='$db'"

    log_debug "$prefix: local $user@$host:$port/$db"
}

parse_ssh_connection() {
    local conn="$1"
    local prefix="$2"

    conn="${conn#ssh://}"

    local ssh_user=""
    local ssh_host=""
    local ssh_port="22"
    local db_user=""
    local db_host="localhost"
    local db_port="5432"
    local db=""

    # Format: ssh_user@ssh_host[:ssh_port]/db_user@db_host[:db_port]/database
    # Examples:
    #   awesome@10.10.1.30/postgres@localhost/workarea
    #   awesome@10.10.1.30:2222/postgres@localhost:5433/workarea

    # Extract database name (last /)
    if [[ "$conn" == */* ]]; then
        db="${conn##*/}"
        conn="${conn%/*}"
    else
        log_error "Missing database in SSH connection"
        return 1
    fi

    # Check if there's still a / (means db_user@db_host part exists)
    if [[ "$conn" == */* ]]; then
        # Has db connection part
        local db_part="${conn##*/}"
        conn="${conn%/*}"

        # Parse db_part: user@host[:port]
        if [[ "$db_part" == *@* ]]; then
            db_user="${db_part%%@*}"
            local db_host_port="${db_part#*@}"

            if [[ "$db_host_port" == *:* ]]; then
                db_host="${db_host_port%%:*}"
                db_port="${db_host_port##*:}"
            else
                db_host="$db_host_port"
            fi
        else
            # Just host or host:port
            if [[ "$db_part" == *:* ]]; then
                db_host="${db_part%%:*}"
                db_port="${db_part##*:}"
            else
                db_host="$db_part"
            fi
        fi
    fi

    # SSH part: [user@]host[:port]
    if [[ "$conn" == *@* ]]; then
        ssh_user="${conn%%@*}"
        conn="${conn#*@}"
    else
        ssh_user="$(whoami)"
    fi

    if [[ "$conn" == *:* ]]; then
        ssh_host="${conn%%:*}"
        ssh_port="${conn##*:}"
    else
        ssh_host="$conn"
    fi

    # If db_user not specified, use "postgres" as default
    [ -z "$db_user" ] && db_user="postgres"

    eval "${prefix}_TYPE='ssh'"
    eval "${prefix}_SSH_USER='$ssh_user'"
    eval "${prefix}_SSH_HOST='$ssh_host'"
    eval "${prefix}_SSH_PORT='$ssh_port'"
    eval "${prefix}_DB_USER='$db_user'"
    eval "${prefix}_DB_HOST='$db_host'"
    eval "${prefix}_DB_PORT='$db_port'"
    eval "${prefix}_DATABASE='$db'"

    log_debug "$prefix: ssh $ssh_user@$ssh_host:$ssh_port -> $db_user@$db_host:$db_port/$db"
}

# ==============================================================================
# PASSWORD HANDLER
# ==============================================================================
get_password() {
    local prefix="$1"
    local index="${2:-0}"  # Optional index for multiple targets
    local pass=""

    # 1. Direct parameter
    if [ "$prefix" = "FROM" ] && [ -n "$FROM_PASSWORD" ]; then
        pass="$FROM_PASSWORD"
    elif [ "$prefix" = "TO" ] && [ -n "$TO_PASSWORD" ]; then
        pass="$TO_PASSWORD"
    elif [ -n "$PASSWORD" ]; then
        pass="$PASSWORD"
    fi

    # 2. Environment variable
    if [ -z "$pass" ]; then
        if [ "$prefix" = "FROM" ] && [ -n "$T_PGSQL_FROM_PASSWORD" ]; then
            pass="$T_PGSQL_FROM_PASSWORD"
        elif [ "$prefix" = "TO" ] && [ -n "$T_PGSQL_TO_PASSWORD" ]; then
            pass="$T_PGSQL_TO_PASSWORD"
        elif [ -n "$T_PGSQL_PASSWORD" ]; then
            pass="$T_PGSQL_PASSWORD"
        fi
    fi

    # 3. Password file
    if [ -z "$pass" ]; then
        local pf=""
        if [ "$prefix" = "FROM" ] && [ -n "$FROM_PASSWORD_FILE" ]; then
            pf="$FROM_PASSWORD_FILE"
        elif [ "$prefix" = "TO" ] && [ ${#TO_PASSWORD_FILES[@]} -gt 0 ]; then
            # Multiple targets: use indexed password file or fallback to first one
            if [ ${#TO_PASSWORD_FILES[@]} -gt 1 ] && [ -n "${TO_PASSWORD_FILES[$index]}" ]; then
                pf="${TO_PASSWORD_FILES[$index]}"
            else
                # Single password file for all targets
                pf="${TO_PASSWORD_FILES[0]}"
            fi
        elif [ -n "$PASSWORD_FILE" ]; then
            pf="$PASSWORD_FILE"
        fi

        if [ -n "$pf" ] && [ -f "$pf" ]; then
            pass=$(cat "$pf" | tr -d '\n')
            log_debug "Password from file: $pf"
        fi
    fi

    # 4. Interactive prompt
    if [ -z "$pass" ] && [ -t 0 ] && [ "$YES" != true ]; then
        read -s -p "$prefix password: " pass
        echo ""
    fi

    # Sudo mode doesn't need password for FROM
    if [ -z "$pass" ] && [ "$prefix" = "FROM" ] && [ "$SUDO" = true ]; then
        log_debug "Sudo mode: skipping FROM password"
        eval "${prefix}_DB_PASSWORD=''"
        return 0
    fi

    if [ -z "$pass" ]; then
        log_error "No password for $prefix"
        return 1
    fi

    eval "${prefix}_DB_PASSWORD='$pass'"
}

# ==============================================================================
# HEALTH CHECK
# ==============================================================================
health_check() {
    local prefix="$1"  # FROM or TO
    local label="${2:-$prefix}"

    local type="" host="" port="" user="" password="" database=""

    if [ "$prefix" = "FROM" ]; then
        type="$FROM_TYPE"
        host="${FROM_SSH_HOST:-$FROM_DB_HOST}"
        port="$FROM_DB_PORT"
        user="$FROM_DB_USER"
        password="$FROM_DB_PASSWORD"
        database="$FROM_DATABASE"
    else
        type="$TO_TYPE"
        host="${TO_SSH_HOST:-$TO_DB_HOST}"
        port="$TO_DB_PORT"
        user="$TO_DB_USER"
        password="$TO_DB_PASSWORD"
        database="$TO_DATABASE"
    fi

    log_info "Health check: $label ($host/$database)..."

    local result=0

    if [ "$type" = "ssh" ]; then
        local ssh_user ssh_host ssh_port db_host db_port
        if [ "$prefix" = "FROM" ]; then
            ssh_user="$FROM_SSH_USER"
            ssh_host="$FROM_SSH_HOST"
            ssh_port="$FROM_SSH_PORT"
            db_host="$FROM_DB_HOST"
            db_port="$FROM_DB_PORT"
        else
            ssh_user="$TO_SSH_USER"
            ssh_host="$TO_SSH_HOST"
            ssh_port="$TO_SSH_PORT"
            db_host="$TO_DB_HOST"
            db_port="$TO_DB_PORT"
        fi

        # Check SSH connection
        if ! ssh -p "$ssh_port" -o ConnectTimeout=5 -o BatchMode=yes "${ssh_user}@${ssh_host}" "echo ok" >/dev/null 2>&1; then
            log_error "SSH connection failed: ${ssh_user}@${ssh_host}:${ssh_port}"
            return 1
        fi
        log_debug "SSH connection OK"

        # Check database connection via SSH
        local check_cmd="env PGPASSWORD='$password' psql -U $user -h $db_host -p $db_port -d $database -c 'SELECT 1' >/dev/null 2>&1 && echo ok"
        if ! ssh -p "$ssh_port" "${ssh_user}@${ssh_host}" "$check_cmd" | grep -q "ok"; then
            # If database doesn't exist, try connecting to postgres database to verify server access
            local fallback_cmd="env PGPASSWORD='$password' psql -U $user -h $db_host -p $db_port -d postgres -c 'SELECT 1' >/dev/null 2>&1 && echo ok"
            if ! ssh -p "$ssh_port" "${ssh_user}@${ssh_host}" "$fallback_cmd" | grep -q "ok"; then
                log_error "Database connection failed: $user@$db_host:$db_port/$database"
                return 1
            fi
            log_debug "Database '$database' doesn't exist yet, but PostgreSQL server is accessible"
        fi
    else
        # Local database check
        if ! env PGPASSWORD="$password" psql -U "$user" -h "$FROM_DB_HOST" -p "$port" -d "$database" -c 'SELECT 1' >/dev/null 2>&1; then
            # If database doesn't exist, try connecting to postgres database to verify server access
            if ! env PGPASSWORD="$password" psql -U "$user" -h "$FROM_DB_HOST" -p "$port" -d postgres -c 'SELECT 1' >/dev/null 2>&1; then
                log_error "Database connection failed: $user@$FROM_DB_HOST:$port/$database"
                return 1
            fi
            log_debug "Database '$database' doesn't exist yet, but PostgreSQL server is accessible"
        fi
    fi

    log_success "Health check passed: $label"
    return 0
}

run_health_checks() {
    local check_from="${1:-true}"
    local check_to="${2:-false}"

    if [ "$HEALTH_CHECK" != true ]; then
        return 0
    fi

    local failed=0

    if [ "$check_from" = true ] && [ -n "$FROM_DATABASE" ]; then
        health_check "FROM" "Source" || failed=1
    fi

    if [ "$check_to" = true ] && [ -n "$TO_DATABASE" ]; then
        health_check "TO" "Target" || failed=1
    fi

    if [ $failed -eq 1 ] && [ "$HEALTH_CHECK_FAIL" = true ]; then
        log_error "Health check failed, aborting (--health-check-fail)"
        exit 1
    fi

    return $failed
}

# ==============================================================================
# CONFIG FILE PARSER
# ==============================================================================
load_config() {
    local config_file="$1"

    if [ ! -f "$config_file" ]; then
        log_error "Config file not found: $config_file"
        return 1
    fi

    log_debug "Loading config: $config_file"

    # Parse YAML-like config file
    while IFS=': ' read -r key value || [ -n "$key" ]; do
        # Skip comments and empty lines
        [[ "$key" =~ ^#.*$ || -z "$key" ]] && continue

        # Trim whitespace and quotes
        key=$(echo "$key" | xargs)
        value=$(echo "$value" | sed 's/^["'"'"']\|["'"'"']$//g' | xargs)

        # Expand ~ to home directory
        value="${value/#\~/$HOME}"

        case "$key" in
            from) [ -z "$FROM_CONNECTION" ] && FROM_CONNECTION="$value" ;;
            to) [ ${#TO_CONNECTIONS[@]} -eq 0 ] && TO_CONNECTIONS+=("$value") ;;
            password) [ -z "$PASSWORD" ] && PASSWORD="$value" ;;
            from_password|from-password) [ -z "$FROM_PASSWORD" ] && FROM_PASSWORD="$value" ;;
            to_password|to-password) [ -z "$TO_PASSWORD" ] && TO_PASSWORD="$value" ;;
            password_file|password-file) [ -z "$PASSWORD_FILE" ] && PASSWORD_FILE="$value" ;;
            from_password_file|from-password-file) [ -z "$FROM_PASSWORD_FILE" ] && FROM_PASSWORD_FILE="$value" ;;
            to_password_file|to-password-file) [ ${#TO_PASSWORD_FILES[@]} -eq 0 ] && TO_PASSWORD_FILES+=("$value") ;;
            output) [ "$OUTPUT_DIR" = "${SCRIPT_DIR}/../data/dumps" ] && OUTPUT_DIR="$value" ;;
            keep) [ "$KEEP" -eq -1 ] && KEEP="$value" ;;
            from_keep|from-keep) [ "$FROM_KEEP" -eq 1 ] && FROM_KEEP="$value" ;;
            compress) [ "$COMPRESS" = "gzip" ] && COMPRESS="$value" ;;
            exclude_table|exclude-table) [ -z "$EXCLUDE_TABLES" ] && EXCLUDE_TABLES="$value" ;;
            exclude_schema|exclude-schema) [ -z "$EXCLUDE_SCHEMAS" ] && EXCLUDE_SCHEMAS="$value" ;;
            exclude_data|exclude-data) [ -z "$EXCLUDE_DATA" ] && EXCLUDE_DATA="$value" ;;
            only_table|only-table) [ -z "$ONLY_TABLES" ] && ONLY_TABLES="$value" ;;
            only_schema|only-schema) [ -z "$ONLY_SCHEMAS" ] && ONLY_SCHEMAS="$value" ;;
            notify) NOTIFY+=("$value") ;;
            verbose) [ "$value" = "true" ] && VERBOSE=true ;;
            force) [ "$value" = "true" ] && FORCE=true ;;
            sudo) [ "$value" = "true" ] && SUDO=true ;;
        esac
    done < "$config_file"

    log_debug "Config loaded"
}

# ==============================================================================
# DUMP
# ==============================================================================
cmd_dump() {
    meta_start
    log_info "Starting dump..."

    if [ -z "$FROM_CONNECTION" ]; then
        log_error "--from required"
        return 1
    fi

    parse_connection "$FROM_CONNECTION" "FROM"
    get_password "FROM"

    # Check if recent dump exists (skip_if_recent)
    if check_skip_recent; then
        return 0
    fi

    # Run health check before operation
    run_health_checks true false

    mkdir -p "$OUTPUT_DIR"

    local ts=$(date '+%Y%m%d_%H%M%S')
    local dump_base_name="${DUMP_NAME:-$FROM_DATABASE}"
    local dump_file="${OUTPUT_DIR}/${dump_base_name}_${ts}.dump"

    log_info "From: ${FROM_DB_USER}@${FROM_DB_HOST}:${FROM_DB_PORT}/${FROM_DATABASE}"
    log_info "To: ${dump_file}"

    if [ "$DRY_RUN" = true ]; then
        log_info "[DRY-RUN] Would dump to: $dump_file"
        return 0
    fi

    # Build exclude options
    local exclude_opts=""
    if [ -n "$EXCLUDE_TABLES" ]; then
        IFS=',' read -ra arr <<< "$EXCLUDE_TABLES"
        for t in "${arr[@]}"; do
            exclude_opts="$exclude_opts --exclude-table='$(echo $t | xargs)'"
        done
    fi
    if [ -n "$EXCLUDE_SCHEMAS" ]; then
        IFS=',' read -ra arr <<< "$EXCLUDE_SCHEMAS"
        for s in "${arr[@]}"; do
            exclude_opts="$exclude_opts --exclude-schema='$(echo $s | xargs)'"
        done
    fi
    if [ -n "$EXCLUDE_DATA" ]; then
        IFS=',' read -ra arr <<< "$EXCLUDE_DATA"
        for t in "${arr[@]}"; do
            t=$(echo $t | xargs)
            # Check for wildcard pattern (e.g., "public.*" or "audit.*")
            if [[ "$t" == *".*" ]]; then
                local schema_name="${t%.*}"
                log_info "Expanding wildcard: $t"
                local tables=""
                if [ "$FROM_TYPE" = "ssh" ]; then
                    tables=$(ssh -p "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}" \
                        "env PGPASSWORD='$FROM_DB_PASSWORD' psql -U $FROM_DB_USER -h $FROM_DB_HOST -p $FROM_DB_PORT -d $FROM_DATABASE -tAc \"SELECT schemaname || '.' || tablename FROM pg_tables WHERE schemaname='$schema_name'\"")
                else
                    tables=$(env PGPASSWORD="$FROM_DB_PASSWORD" psql -U $FROM_DB_USER -h $FROM_DB_HOST -p $FROM_DB_PORT -d $FROM_DATABASE -tAc "SELECT schemaname || '.' || tablename FROM pg_tables WHERE schemaname='$schema_name'")
                fi
                for tbl in $tables; do
                    exclude_opts="$exclude_opts --exclude-table-data='$tbl'"
                done
            else
                exclude_opts="$exclude_opts --exclude-table-data='$t'"
            fi
        done
    fi

    local result=0

    if [ "$FROM_TYPE" = "ssh" ]; then
        log_info "Dumping via SSH..."
        local remote_dump_dir="/tmp/t-pgsql"
        ssh -p "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}" "mkdir -p '$remote_dump_dir'"
        local dump_base_name="${DUMP_NAME:-$FROM_DATABASE}"
        local remote_file="${remote_dump_dir}/${dump_base_name}_${ts}.dump"

        local cmd=""
        if [ "$SUDO" = true ]; then
            # Use sudo -u postgres (peer auth)
            cmd="sudo -u $FROM_DB_USER pg_dump"
            cmd="$cmd -h $FROM_DB_HOST -p $FROM_DB_PORT"
        else
            # Use password auth
            cmd="env PGPASSWORD='$FROM_DB_PASSWORD' pg_dump"
            cmd="$cmd -U $FROM_DB_USER -h $FROM_DB_HOST -p $FROM_DB_PORT"
        fi
        cmd="$cmd -Fc -Z${PG_COMPRESS_LEVEL} -v $exclude_opts"
        cmd="$cmd -f '$remote_file' $FROM_DATABASE"

        ssh -p "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}" "$cmd"
        result=$?

        if [ $result -eq 0 ]; then
            log_info "Transferring..."
            scp -P "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}:${remote_file}" "$dump_file"

            # Cleanup based on FROM_KEEP
            if [ "$FROM_KEEP" -eq 0 ]; then
                log_info "Cleaning source dump..."
                ssh -p "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}" "rm -f '$remote_file'"
            elif [ "$FROM_KEEP" -gt 0 ]; then
                log_info "Keeping last $FROM_KEEP dump(s) on source..."
                local dump_base_name="${DUMP_NAME:-$FROM_DATABASE}"
                local cleanup_cmd="cd /tmp/t-pgsql && ls -t ${dump_base_name}_*.dump 2>/dev/null | tail -n +$((FROM_KEEP + 1)) | xargs -r rm -f"
                ssh -p "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}" "$cleanup_cmd"
            else
                log_info "Keeping source dump (--from-keep -1)"
            fi
        fi
    else
        log_info "Dumping locally..."
        export PGPASSWORD="$FROM_DB_PASSWORD"

        local cmd="pg_dump -U $FROM_DB_USER -h $FROM_DB_HOST -p $FROM_DB_PORT"
        cmd="$cmd -Fc -Z${PG_COMPRESS_LEVEL} -v $exclude_opts"
        cmd="$cmd -f '$dump_file' $FROM_DATABASE"

        eval "$cmd"
        result=$?
        unset PGPASSWORD
    fi

    if [ $result -eq 0 ]; then
        local size=$(ls -lh "$dump_file" | awk '{print $5}')
        local elapsed=$(format_elapsed $(($(date +%s) - META_START_EPOCH)))
        log_success "Dump complete: $dump_file ($size)"

        # External compression
        if [ "$COMPRESS" != "gzip" ] && [ "$COMPRESS" != "none" ]; then
            compress_file "$dump_file"
        fi

        # Write metadata
        meta_write "$dump_file" "success" 0

        # Cleanup
        cleanup_old_dumps

        # Send notification
        local details=$(build_notify_details "DUMP" "Success" "$elapsed" "$size")
        send_notification "success" "Dump completed: ${FROM_DATABASE}" "$details"

        echo "DUMP_FILE=$dump_file"
        echo "DUMP_SIZE=$size"
    else
        log_error "Dump failed: $result"
        meta_write "$dump_file" "failed" $result

        # Send notification
        local elapsed=$(format_elapsed $(($(date +%s) - META_START_EPOCH)))
        local details=$(build_notify_details "DUMP" "Failed" "$elapsed")
        send_notification "failed" "Dump failed: ${FROM_DATABASE}" "$details"

        return 1
    fi
}

# ==============================================================================
# RESTORE
# ==============================================================================
cmd_restore() {
    log_info "Starting restore..."

    # Auto-find latest dump if no file specified
    if [ -z "$FILE" ]; then
        local norm_dir=$(cd "$OUTPUT_DIR" 2>/dev/null && pwd)
        FILE=$(ls -t "${norm_dir}/"*.tar.gz 2>/dev/null | head -1)
        [ -z "$FILE" ] && FILE=$(ls -t "${norm_dir}/"*.dump 2>/dev/null | head -1)

        if [ -z "$FILE" ]; then
            log_error "No dump file found. Use --file <path>"
            return 1
        fi
        log_info "Using latest: $(basename "$FILE")"
    fi

    if [ ! -f "$FILE" ]; then
        log_error "File not found: $FILE"
        return 1
    fi

    if [ ${#TO_CONNECTIONS[@]} -eq 0 ]; then
        log_error "--to required"
        return 1
    fi

    local restore_file="$FILE"
    local temp_dir=""
    local cleanup_temp=false

    # Handle different archive types
    case "$restore_file" in
        *.tar.gz)
            # Extract dump from tar archive
            temp_dir=$(mktemp -d)
            cleanup_temp=true
            log_info "Extracting from archive..."
            restore_file=$(extract_dump "$FILE" "$temp_dir")
            if [ -z "$restore_file" ] || [ ! -f "$restore_file" ]; then
                log_error "Failed to extract dump from archive"
                rm -rf "$temp_dir"
                return 1
            fi
            ;;
        *.gz)  gunzip -k "$restore_file"; restore_file="${restore_file%.gz}" ;;
        *.xz)  unxz -k "$restore_file"; restore_file="${restore_file%.xz}" ;;
        *.bz2) bunzip2 -k "$restore_file"; restore_file="${restore_file%.bz2}" ;;
        *.zst) zstd -dk "$restore_file"; restore_file="${restore_file%.zst}" ;;
    esac

    local idx=0
    local restore_failed=0
    for conn in "${TO_CONNECTIONS[@]}"; do
        restore_to "$conn" "$restore_file" "$idx" || restore_failed=1
        ((idx++))
    done

    # Cleanup temp dir if created
    [ "$cleanup_temp" = true ] && rm -rf "$temp_dir"

    # Send notification (only for standalone restore, not when called from clone)
    if [ "$COMMAND" = "restore" ]; then
        if [ $restore_failed -eq 0 ]; then
            local details=$(build_notify_details "RESTORE" "Success" "")
            send_notification "success" "Restore completed: ${TO_DATABASE}" "$details"
        else
            local details=$(build_notify_details "RESTORE" "Failed" "")
            send_notification "failed" "Restore failed: ${TO_DATABASE}" "$details"
        fi
    fi

    return $restore_failed
}

restore_to() {
    local conn="$1"
    local dump="$2"
    local index="${3:-0}"

    parse_connection "$conn" "TO"
    get_password "TO" "$index"

    log_info "To: ${TO_DB_USER}@${TO_DB_HOST}:${TO_DB_PORT}/${TO_DATABASE}"

    if [ "$DRY_RUN" = true ]; then
        log_info "[DRY-RUN] Would restore to: $conn"
        return 0
    fi

    if [ "$TO_TYPE" = "ssh" ]; then
        restore_ssh "$dump"
    else
        restore_local "$dump"
    fi
}

restore_local() {
    local dump="$1"

    export PGPASSWORD="$TO_DB_PASSWORD"

    # Check exists
    local exists=$(psql -U "$TO_DB_USER" -h "$TO_DB_HOST" -p "$TO_DB_PORT" \
        -tAc "SELECT 1 FROM pg_database WHERE datname='$TO_DATABASE'" postgres 2>/dev/null)

    if [ "$exists" = "1" ]; then
        if [ "$FORCE" = true ]; then
            log_warn "Database '$TO_DATABASE' exists, dropping (--force)..."
            psql -U "$TO_DB_USER" -h "$TO_DB_HOST" -p "$TO_DB_PORT" \
                -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='$TO_DATABASE' AND pid<>pg_backend_pid();" postgres >/dev/null 2>&1
            dropdb -U "$TO_DB_USER" -h "$TO_DB_HOST" -p "$TO_DB_PORT" "$TO_DATABASE" 2>/dev/null
        else
            log_error "Database '$TO_DATABASE' already exists. Use --force to overwrite."
            unset PGPASSWORD
            return 1
        fi
    fi

    log_info "Creating database..."
    createdb -U "$TO_DB_USER" -h "$TO_DB_HOST" -p "$TO_DB_PORT" "$TO_DATABASE"

    log_info "Restoring..."
    pg_restore --verbose --no-owner --no-privileges --clean --if-exists \
        -U "$TO_DB_USER" -h "$TO_DB_HOST" -p "$TO_DB_PORT" \
        -d "$TO_DATABASE" "$dump" 2>&1

    local r=$?
    unset PGPASSWORD

    # Apply data masking if enabled
    apply_masking "TO"

    [ $r -eq 0 ] && log_success "Restore complete" || log_warn "Restore done with warnings"
}

restore_ssh() {
    local dump="$1"
    ssh -p "$TO_SSH_PORT" "${TO_SSH_USER}@${TO_SSH_HOST}" "mkdir -p /tmp/t-pgsql"
    local remote="/tmp/t-pgsql/$(basename $dump)"

    # Check if DB exists on remote
    local check_cmd="env PGPASSWORD='$TO_DB_PASSWORD' psql -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT -tAc \"SELECT 1 FROM pg_database WHERE datname='$TO_DATABASE'\" postgres 2>/dev/null"
    local exists=$(ssh -p "$TO_SSH_PORT" "${TO_SSH_USER}@${TO_SSH_HOST}" "$check_cmd")

    if [ "$exists" = "1" ]; then
        if [ "$FORCE" = true ]; then
            log_warn "Remote database '$TO_DATABASE' exists, will drop (--force)..."
        else
            log_error "Remote database '$TO_DATABASE' already exists. Use --force to overwrite."
            return 1
        fi
    fi

    log_info "Uploading dump..."
    scp -P "$TO_SSH_PORT" "$dump" "${TO_SSH_USER}@${TO_SSH_HOST}:${remote}"

    log_info "Restoring on remote..."
    local cmd="export PGPASSWORD='$TO_DB_PASSWORD';"
    if [ "$FORCE" = true ]; then
        cmd+="psql -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT -c \"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='$TO_DATABASE' AND pid<>pg_backend_pid();\" postgres 2>/dev/null;"
        cmd+="dropdb -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT --if-exists $TO_DATABASE 2>/dev/null;"
    fi
    cmd+="createdb -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT $TO_DATABASE 2>/dev/null || true;"
    cmd+="pg_restore --verbose --no-owner --no-privileges --clean --if-exists -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT -d $TO_DATABASE '$remote';"
    cmd+="rm -f '$remote'"

    ssh -p "$TO_SSH_PORT" "${TO_SSH_USER}@${TO_SSH_HOST}" "$cmd"

    # Apply data masking if enabled
    apply_masking "TO"

    [ $? -eq 0 ] && log_success "Remote restore complete" || log_warn "Remote restore done with warnings"
}

# ==============================================================================
# FETCH (existing dump from source)
# ==============================================================================
cmd_fetch() {
    meta_start
    log_info "Fetching existing dump..."

    if [ -z "$FROM_CONNECTION" ]; then
        log_error "--from required"
        return 1
    fi

    parse_connection "$FROM_CONNECTION" "FROM"

    if [ "$FROM_TYPE" != "ssh" ]; then
        log_error "fetch command requires SSH connection (ssh://...)"
        return 1
    fi

    mkdir -p "$OUTPUT_DIR"

    # Determine which file to fetch
    local remote_file=""

    if [ -n "$FROM_FILE" ] && [ "$FROM_FILE" != "latest" ]; then
        # User specified file/pattern
        if [[ "$FROM_FILE" == *"*"* ]]; then
            # Glob pattern - find latest matching
            log_info "Finding latest match for: $FROM_FILE"
            remote_file=$(ssh -p "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}" \
                "ls -t /tmp/t-pgsql/$FROM_FILE 2>/dev/null | head -1")
        else
            # Exact filename - check if it has path
            if [[ "$FROM_FILE" == /* ]]; then
                remote_file="$FROM_FILE"
            else
                remote_file="/tmp/t-pgsql/$FROM_FILE"
            fi
        fi
    else
        # Auto-find latest dump for database (--from-file or --from-file latest)
        log_info "Finding latest dump for: $FROM_DATABASE"
        remote_file=$(ssh -p "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}" \
            "ls -t /tmp/t-pgsql/${FROM_DATABASE}_*.dump 2>/dev/null | head -1")
    fi

    if [ -z "$remote_file" ]; then
        log_error "No dump file found on source"
        return 1
    fi

    # Check file exists
    local exists=$(ssh -p "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}" \
        "[ -f '$remote_file' ] && echo 'yes' || echo 'no'")

    if [ "$exists" != "yes" ]; then
        log_error "File not found: $remote_file"
        return 1
    fi

    log_info "Found: $remote_file"

    if [ "$DRY_RUN" = true ]; then
        log_info "[DRY-RUN] Would fetch: $remote_file"
        return 0
    fi

    local local_file="${OUTPUT_DIR}/$(basename $remote_file)"

    log_info "Downloading..."
    scp -P "$FROM_SSH_PORT" "${FROM_SSH_USER}@${FROM_SSH_HOST}:${remote_file}" "$local_file"

    if [ $? -eq 0 ]; then
        local size=$(ls -lh "$local_file" | awk '{print $5}')
        log_success "Fetched: $local_file ($size)"
        meta_write "$local_file" "success" 0
        
        # Send notification
        local details=$(build_notify_details "FETCH" "Success" "File: $(basename $local_file)\nSize: $size\nSource: ${FROM_SSH_HOST}")
        send_notification "success" "Fetch completed: ${FROM_DATABASE}" "$details"
        
        echo "DUMP_FILE=$local_file"
        echo "DUMP_SIZE=$size"
    else
        log_error "Fetch failed"
        meta_write "$local_file" "failed" 1
        
        # Send notification
        local details=$(build_notify_details "FETCH" "Failed" "Source: ${FROM_SSH_HOST}")
        send_notification "failed" "Fetch failed: ${FROM_DATABASE}" "$details"
        
        return 1
    fi
}

# ==============================================================================
# DATA MASKING
# ==============================================================================
apply_masking() {
    local prefix="$1"  # TO connection prefix

    if [ "$MASK" != true ]; then
        return 0
    fi

    log_info "Applying data masking..."

    local type="" host="" port="" user="" password="" database=""

    if [ "$prefix" = "TO" ]; then
        type="$TO_TYPE"
        host="$TO_DB_HOST"
        port="$TO_DB_PORT"
        user="$TO_DB_USER"
        password="$TO_DB_PASSWORD"
        database="$TO_DATABASE"
    fi

    local mask_sql=""

    # Load rules from file if specified
    if [ -n "$MASK_RULES" ] && [ -f "$MASK_RULES" ]; then
        log_debug "Loading mask rules from: $MASK_RULES"

        # Parse JSON rules file
        # Format: {"table.column": "SQL_EXPRESSION", ...}
        while IFS=': ' read -r key value; do
            [[ "$key" =~ ^[[:space:]]*[\{\}] ]] && continue
            [[ -z "$key" ]] && continue

            # Clean up JSON formatting
            key=$(echo "$key" | sed 's/^[[:space:]]*"//; s/"[[:space:]]*$//')
            value=$(echo "$value" | sed 's/^[[:space:]]*"//; s/"[[:space:]]*,*$//')

            [ -z "$key" ] || [ -z "$value" ] && continue

            local table=$(echo "$key" | cut -d. -f1)
            local column=$(echo "$key" | cut -d. -f2)

            mask_sql+="UPDATE $table SET $column = $value;"
            log_debug "Mask rule: $table.$column = $value"
        done < "$MASK_RULES"
    fi

    # Apply masking to specific tables if --mask-tables specified
    if [ -n "$MASK_TABLES" ]; then
        IFS=',' read -ra tables <<< "$MASK_TABLES"
        for table in "${tables[@]}"; do
            table=$(echo "$table" | xargs)

            # Default masking rules for common columns
            mask_sql+="UPDATE $table SET email = CONCAT(LEFT(email, 2), '***@***.com') WHERE email IS NOT NULL;"
            mask_sql+="UPDATE $table SET phone = '***-***-****' WHERE phone IS NOT NULL;"
            mask_sql+="UPDATE $table SET password = '********' WHERE password IS NOT NULL;"
            mask_sql+="UPDATE $table SET password_hash = 'MASKED' WHERE password_hash IS NOT NULL;"
            mask_sql+="UPDATE $table SET address = '[MASKED]' WHERE address IS NOT NULL;"
            mask_sql+="UPDATE $table SET ssn = '***-**-****' WHERE ssn IS NOT NULL;"
            mask_sql+="UPDATE $table SET credit_card = '****-****-****-****' WHERE credit_card IS NOT NULL;"

            log_debug "Auto-masking table: $table"
        done
    fi

    [ -z "$mask_sql" ] && { log_warn "No masking rules defined"; return 0; }

    # Execute masking SQL
    local result=0
    if [ "$type" = "ssh" ]; then
        local cmd="env PGPASSWORD='$password' psql -U $user -h $host -p $port -d $database -c \"$mask_sql\" 2>&1"
        ssh -p "$TO_SSH_PORT" "${TO_SSH_USER}@${TO_SSH_HOST}" "$cmd"
        result=$?
    else
        env PGPASSWORD="$password" psql -U "$user" -h "$host" -p "$port" -d "$database" -c "$mask_sql" 2>&1
        result=$?
    fi

    if [ $result -eq 0 ]; then
        log_success "Data masking applied"
    else
        log_warn "Data masking completed with warnings"
    fi

    return 0
}

# ==============================================================================
# STREAMING CLONE (no temp files)
# ==============================================================================
clone_stream() {
    meta_start
    log_info "Starting streaming clone (no temp files)..."

    local exclude_opts=""
    if [ -n "$EXCLUDE_TABLES" ]; then
        IFS=',' read -ra arr <<< "$EXCLUDE_TABLES"
        for t in "${arr[@]}"; do
            exclude_opts="$exclude_opts --exclude-table='$(echo $t | xargs)'"
        done
    fi
    if [ -n "$EXCLUDE_SCHEMAS" ]; then
        IFS=',' read -ra arr <<< "$EXCLUDE_SCHEMAS"
        for s in "${arr[@]}"; do
            exclude_opts="$exclude_opts --exclude-schema='$(echo $s | xargs)'"
        done
    fi

    local result=0
    local idx=0

    for to_conn in "${TO_CONNECTIONS[@]}"; do
        parse_connection "$to_conn" "TO"
        get_password "TO" "$idx"

        log_info "Streaming: ${FROM_SSH_HOST:-$FROM_DB_HOST}/${FROM_DATABASE} ‚Üí ${TO_SSH_HOST:-$TO_DB_HOST}/${TO_DATABASE}"

        # Build pg_dump command
        local dump_cmd=""
        if [ "$FROM_TYPE" = "ssh" ]; then
            if [ "$SUDO" = true ]; then
                dump_cmd="ssh -p $FROM_SSH_PORT ${FROM_SSH_USER}@${FROM_SSH_HOST} 'sudo -u $FROM_DB_USER pg_dump -h $FROM_DB_HOST -p $FROM_DB_PORT -Fc $exclude_opts $FROM_DATABASE'"
            else
                dump_cmd="ssh -p $FROM_SSH_PORT ${FROM_SSH_USER}@${FROM_SSH_HOST} 'env PGPASSWORD=\"$FROM_DB_PASSWORD\" pg_dump -U $FROM_DB_USER -h $FROM_DB_HOST -p $FROM_DB_PORT -Fc $exclude_opts $FROM_DATABASE'"
            fi
        else
            dump_cmd="env PGPASSWORD='$FROM_DB_PASSWORD' pg_dump -U $FROM_DB_USER -h $FROM_DB_HOST -p $FROM_DB_PORT -Fc $exclude_opts $FROM_DATABASE"
        fi

        # Build pg_restore command
        local restore_cmd=""
        if [ "$TO_TYPE" = "ssh" ]; then
            # Prepare target database
            local prep_cmd="export PGPASSWORD='$TO_DB_PASSWORD';"
            if [ "$FORCE" = true ]; then
                prep_cmd+="psql -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT -c \"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='$TO_DATABASE' AND pid<>pg_backend_pid();\" postgres 2>/dev/null;"
                prep_cmd+="dropdb -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT --if-exists $TO_DATABASE 2>/dev/null;"
            fi
            prep_cmd+="createdb -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT $TO_DATABASE 2>/dev/null || true"

            ssh -p "$TO_SSH_PORT" "${TO_SSH_USER}@${TO_SSH_HOST}" "$prep_cmd"

            restore_cmd="ssh -p $TO_SSH_PORT ${TO_SSH_USER}@${TO_SSH_HOST} 'env PGPASSWORD=\"$TO_DB_PASSWORD\" pg_restore --no-owner --no-privileges -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT -d $TO_DATABASE'"
        else
            # Prepare local target database
            export PGPASSWORD="$TO_DB_PASSWORD"
            if [ "$FORCE" = true ]; then
                psql -U "$TO_DB_USER" -h "$TO_DB_HOST" -p "$TO_DB_PORT" \
                    -c "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname='$TO_DATABASE' AND pid<>pg_backend_pid();" postgres >/dev/null 2>&1
                dropdb -U "$TO_DB_USER" -h "$TO_DB_HOST" -p "$TO_DB_PORT" --if-exists "$TO_DATABASE" 2>/dev/null
            fi
            createdb -U "$TO_DB_USER" -h "$TO_DB_HOST" -p "$TO_DB_PORT" "$TO_DATABASE" 2>/dev/null || true

            restore_cmd="env PGPASSWORD='$TO_DB_PASSWORD' pg_restore --no-owner --no-privileges -U $TO_DB_USER -h $TO_DB_HOST -p $TO_DB_PORT -d $TO_DATABASE"
        fi

        # Execute streaming pipe with buffer
        log_debug "Executing: $dump_cmd | pv -q -B ${STREAM_BUFFER}M | $restore_cmd"

        if command -v pv &>/dev/null; then
            # Use pv for buffering if available
            eval "$dump_cmd" | pv -q -B "${STREAM_BUFFER}M" 2>/dev/null | eval "$restore_cmd"
        else
            # Direct pipe without buffer
            eval "$dump_cmd" | eval "$restore_cmd"
        fi

        local pipe_result=${PIPESTATUS[0]}
        [ $pipe_result -ne 0 ] && result=1

        idx=$((idx + 1))
    done

    local elapsed=$(format_elapsed $(($(date +%s) - META_START_EPOCH)))

    if [ $result -eq 0 ]; then
        log_success "Streaming clone complete ($elapsed)"
        local details=$(build_notify_details "STREAM-CLONE" "Success" "$elapsed")
        send_notification "success" "Streaming clone completed: ${FROM_DATABASE} ‚Üí ${TO_DATABASE}" "$details"
    else
        log_error "Streaming clone failed"
        local details=$(build_notify_details "STREAM-CLONE" "Failed" "$elapsed")
        send_notification "failed" "Streaming clone failed: ${FROM_DATABASE} ‚Üí ${TO_DATABASE}" "$details"
    fi

    return $result
}

# ==============================================================================
# CLONE
# ==============================================================================
cmd_clone() {
    log_info "Starting clone..."

    # Validate connections first
    if [ -z "$FROM_CONNECTION" ]; then
        log_error "--from required"
        return 1
    fi

    if [ ${#TO_CONNECTIONS[@]} -eq 0 ]; then
        log_error "--to required"
        return 1
    fi

    # Parse connections for health check
    parse_connection "$FROM_CONNECTION" "FROM"
    get_password "FROM"

    # Parse first target for health check
    parse_connection "${TO_CONNECTIONS[0]}" "TO"
    get_password "TO" 0

    # Run health check before operation
    run_health_checks true true

    # Check if recent dump exists (skip_if_recent)
    if check_skip_recent; then
        return 0
    fi

    # Dry-run mode
    if [ "$DRY_RUN" = true ]; then
        log_info "[DRY-RUN] Would dump from: $FROM_CONNECTION"
        for conn in "${TO_CONNECTIONS[@]}"; do
            log_info "[DRY-RUN] Would restore to: $conn"
        done
        return 0
    fi

    # Streaming mode: direct pipe without temp files
    if [ "$STREAM" = true ]; then
        clone_stream
        return $?
    fi

    cmd_dump
    [ $? -ne 0 ] && return 1

    # Find latest archive (normalize path first)
    local norm_output_dir=$(cd "$OUTPUT_DIR" && pwd)
    local dump_base_name="${DUMP_NAME:-$FROM_DATABASE}"
    local latest=$(ls -t "${norm_output_dir}/${dump_base_name}_"*.tar.gz 2>/dev/null | head -1)

    # Fallback to .dump if no tar.gz
    [ -z "$latest" ] && latest=$(ls -t "${norm_output_dir}/${dump_base_name}_"*.dump 2>/dev/null | head -1)
    [ -z "$latest" ] && { log_error "Dump not found"; return 1; }

    FILE="$latest"
    local restore_start=$(date +%s)
    cmd_restore
    local r=$?
    local restore_elapsed=$(($(date +%s) - restore_start))

    # Update metadata with target info
    meta_update_target "$latest" $([ $r -eq 0 ] && echo "success" || echo "failed") $restore_elapsed

    # Cleanup
    [ "$KEEP" -eq 0 ] && rm -f "$latest"

    # Send notification
    local total_elapsed=$(format_elapsed $(($(date +%s) - META_START_EPOCH)))
    if [ $r -eq 0 ]; then
        log_success "Clone complete"
        local details=$(build_notify_details "CLONE" "Success" "$total_elapsed")
        send_notification "success" "Clone completed: ${FROM_DATABASE} ‚Üí ${TO_DATABASE}" "$details"
    else
        local details=$(build_notify_details "CLONE" "Failed" "$total_elapsed")
        send_notification "failed" "Clone failed: ${FROM_DATABASE} ‚Üí ${TO_DATABASE}" "$details"
        return 1
    fi
}

# ==============================================================================
# LIST
# ==============================================================================
cmd_list() {
    local norm_dir=$(cd "$OUTPUT_DIR" 2>/dev/null && pwd)
    log_info "Dumps in: $norm_dir"
    echo ""

    [ ! -d "$OUTPUT_DIR" ] && { log_warn "Directory not found"; return 0; }

    printf "%-50s %8s %s\n" "FILE" "SIZE" "DATE"
    printf "%s\n" "$(printf '%.0s-' {1..75})"

    # List both .tar.gz and .dump files
    find "$norm_dir" \( -name "*.tar.gz" -o -name "*.dump" \) -type f -print0 2>/dev/null | \
        xargs -0 stat -f "%m %z %N" 2>/dev/null | \
        sort -rn | \
        while read -r mtime size filepath; do
            local fname=$(basename "$filepath")
            local hsize=$(numfmt --to=iec-i --suffix=B $size 2>/dev/null || echo "${size}B")
            local fdate=$(date -r "$mtime" '+%Y-%m-%d %H:%M' 2>/dev/null || echo "unknown")
            printf "%-50s %8s %s\n" "$fname" "$hsize" "$fdate"
        done
    echo ""
}

# ==============================================================================
# META (show metadata)
# ==============================================================================
cmd_meta() {
    if [ -z "$FILE" ]; then
        # Show latest
        local norm_output_dir=$(cd "$OUTPUT_DIR" 2>/dev/null && pwd)
        FILE=$(ls -t "${norm_output_dir}/"*.tar.gz 2>/dev/null | head -1)
    fi

    if [ -z "$FILE" ] || [ ! -f "$FILE" ]; then
        log_error "No archive found. Use --file <path>"
        return 1
    fi

    log_info "Metadata: $(basename "$FILE")"
    echo ""
    show_meta "$FILE"
}

# ==============================================================================
# CLEAN
# ==============================================================================
cmd_clean() {
    log_info "Cleaning dumps..."
    [ "$DRY_RUN" = true ] && { log_info "[DRY-RUN] Would clean"; return 0; }
    cleanup_old_dumps
    log_success "Done"
}

# ==============================================================================
# BATCH SYSTEM
# ==============================================================================

# Get a value from YAML file using awk
# Usage: get_yaml_value "file" "section" "key"
get_yaml_value() {
    local file="$1"
    local section="$2"
    local key="$3"

    awk -v section="$section" -v key="$key" '
    BEGIN { in_section=0; indent=0 }
    /^[a-zA-Z0-9_-]+:/ {
        if ($1 == section":") { in_section=1; indent=0 }
        else if (in_section && indent==0) { in_section=0 }
    }
    in_section && $1 == key":" {
        gsub(/^[^:]+: */, "")
        gsub(/^["'\'']|["'\'']$/, "")
        print
        exit
    }
    ' "$file"
}

# Get value from defaults section in jobs.yaml
# Usage: get_default_value "key"
get_default_value() {
    local key="$1"

    awk -v key="$key" '
    BEGIN { in_defaults=0 }
    /^defaults:/ { in_defaults=1; next }
    /^[a-zA-Z]/ { if ($0 !~ /^defaults:/) in_defaults=0 }
    in_defaults && /^  [a-zA-Z0-9_-]+:/ {
        gsub(/^ +/, "")
        if ($1 == key":") {
            gsub(/^[^:]+: */, "")
            gsub(/^["'\'']|["'\'']$/, "")
            print
            exit
        }
    }
    ' "$JOBS_FILE"
}

# Get job field value, with fallback to defaults section
# Usage: get_job_value "job_name" "field"
get_job_value() {
    local job="$1"
    local field="$2"
    local val=$(get_job_field "$job" "$field")
    [ -z "$val" ] && val=$(get_default_value "$field")
    echo "$val"
}

# Load batch defaults from jobs.yaml (parallel, continue_on_error)
load_batch_defaults() {
    [ ! -f "$JOBS_FILE" ] && return 0

    local val
    val=$(get_default_value "parallel")
    [ -n "$val" ] && PARALLEL="$val" || true

    val=$(get_default_value "continue_on_error")
    [ "$val" = "true" ] && CONTINUE_ON_ERROR=true || true
}

# Get profile configuration from jobs.yaml
# Usage: get_profile "profile_name" "key"
get_profile_value() {
    local profile="$1"
    local key="$2"

    awk -v profile="$profile" -v key="$key" '
    BEGIN { in_profiles=0; in_profile=0; base_indent=0 }
    /^profiles:/ { in_profiles=1; next }
    /^[a-zA-Z]/ { if ($0 !~ /^profiles:/) { in_profiles=0; in_profile=0 } }
    in_profiles && /^  [a-zA-Z0-9_-]+:/ {
        gsub(/:.*/, "", $1)
        if ($1 == profile) { in_profile=1 }
        else { in_profile=0 }
        next
    }
    in_profile && /^    [a-zA-Z0-9_-]+:/ {
        gsub(/^ +/, "")
        if ($1 == key":") {
            gsub(/^[^:]+: */, "")
            gsub(/^["'\'']|["'\'']$/, "")
            print
            exit
        }
    }
    ' "$JOBS_FILE"
}

# Get job field value (supports nested fields like from.database)
# Usage: get_job_field "job_name" "field" [subfield]
get_job_field() {
    local job="$1"
    local field="$2"
    local subfield="$3"

    if [ -n "$subfield" ]; then
        # Nested field (e.g., from.database)
        awk -v job="$job" -v field="$field" -v subfield="$subfield" '
        BEGIN { in_job=0; in_field=0 }
        /^  [a-zA-Z0-9_-]+:/ {
            gsub(/:.*/, "", $1)
            if ($1 == job) { in_job=1 } else { in_job=0; in_field=0 }
            next
        }
        in_job && /^    [a-zA-Z0-9_-]+:/ {
            gsub(/^ +/, "")
            current_field=$1; gsub(/:.*/, "", current_field)
            if (current_field == field) {
                # Check if value is on same line (string format)
                val=$0; gsub(/^[^:]+: */, "", val)
                if (val != "" && val !~ /^$/) {
                    # Its a string value, not nested
                    if (subfield == "_value_") {
                        gsub(/^["'\'']|["'\'']$/, "", val)
                        print val
                        exit
                    }
                }
                in_field=1
            } else { in_field=0 }
            next
        }
        in_job && in_field && /^      [a-zA-Z0-9_-]+:/ {
            gsub(/^ +/, "")
            if ($1 == subfield":") {
                gsub(/^[^:]+: */, "")
                gsub(/^["'\'']|["'\'']$/, "")
                print
                exit
            }
        }
        ' "$JOBS_FILE"
    else
        # Simple field
        awk -v job="$job" -v field="$field" '
        BEGIN { in_job=0 }
        /^  [a-zA-Z0-9_-]+:/ {
            gsub(/:.*/, "", $1)
            if ($1 == job) { in_job=1 } else { in_job=0 }
            next
        }
        in_job && /^    [a-zA-Z0-9_-]+:/ {
            gsub(/^ +/, "")
            if ($1 == field":") {
                gsub(/^[^:]+: */, "")
                gsub(/^["'\'']|["'\'']$/, "")
                print
                exit
            }
        }
        ' "$JOBS_FILE"
    fi
}

# Build connection string from profile or inline config
# Usage: build_connection "job_name" "from|to"
build_connection() {
    local job="$1"
    local direction="$2"  # from or to

    # First check if it's a simple string value
    local conn_string=$(get_job_field "$job" "$direction" "_value_")
    if [ -n "$conn_string" ]; then
        echo "$conn_string"
        return
    fi

    # Check for profile reference
    local profile=$(get_job_field "$job" "$direction" "profile")

    local type="" ssh_user="" ssh_host="" ssh_port=""
    local db_user="" db_host="" db_port="" database=""

    if [ -n "$profile" ]; then
        # Load from profile
        type=$(get_profile_value "$profile" "type")
        ssh_user=$(get_profile_value "$profile" "ssh_user")
        ssh_host=$(get_profile_value "$profile" "ssh_host")
        ssh_port=$(get_profile_value "$profile" "ssh_port")
        db_user=$(get_profile_value "$profile" "db_user")
        db_host=$(get_profile_value "$profile" "db_host")
        db_port=$(get_profile_value "$profile" "db_port")
    fi

    # Override with job-specific values
    [ -n "$(get_job_field "$job" "$direction" "type")" ] && type=$(get_job_field "$job" "$direction" "type")
    [ -n "$(get_job_field "$job" "$direction" "ssh_user")" ] && ssh_user=$(get_job_field "$job" "$direction" "ssh_user")
    [ -n "$(get_job_field "$job" "$direction" "ssh_host")" ] && ssh_host=$(get_job_field "$job" "$direction" "ssh_host")
    [ -n "$(get_job_field "$job" "$direction" "ssh_port")" ] && ssh_port=$(get_job_field "$job" "$direction" "ssh_port")
    [ -n "$(get_job_field "$job" "$direction" "db_user")" ] && db_user=$(get_job_field "$job" "$direction" "db_user")
    [ -n "$(get_job_field "$job" "$direction" "db_host")" ] && db_host=$(get_job_field "$job" "$direction" "db_host")
    [ -n "$(get_job_field "$job" "$direction" "db_port")" ] && db_port=$(get_job_field "$job" "$direction" "db_port")
    [ -n "$(get_job_field "$job" "$direction" "database")" ] && database=$(get_job_field "$job" "$direction" "database")

    # Set defaults
    [ -z "$db_user" ] && db_user="postgres"
    [ -z "$db_host" ] && db_host="localhost"
    [ -z "$db_port" ] && db_port="5432"
    [ -z "$ssh_port" ] && ssh_port="22"

    # Build connection string
    if [ "$type" = "ssh" ]; then
        local conn="ssh://"
        conn+="${ssh_user}@${ssh_host}"
        [ "$ssh_port" != "22" ] && conn+=":${ssh_port}"
        conn+="/${db_user}@${db_host}"
        [ "$db_port" != "5432" ] && conn+=":${db_port}"
        conn+="/${database}"
        echo "$conn"
    else
        # Local connection
        local conn="${db_user}@${db_host}"
        [ "$db_port" != "5432" ] && conn+=":${db_port}"
        conn+="/${database}"
        echo "$conn"
    fi
}

# Get password file from profile or job config
get_password_file() {
    local job="$1"
    local direction="$2"

    # Check job-level password file first
    local pw_file=$(get_job_field "$job" "${direction}_password_file")
    if [ -n "$pw_file" ]; then
        # Expand ~ to home directory
        echo "${pw_file/#\~/$HOME}"
        return
    fi

    # Check inside direction block
    pw_file=$(get_job_field "$job" "$direction" "password_file")
    if [ -n "$pw_file" ]; then
        echo "${pw_file/#\~/$HOME}"
        return
    fi

    # Check profile
    local profile=$(get_job_field "$job" "$direction" "profile")
    if [ -n "$profile" ]; then
        pw_file=$(get_profile_value "$profile" "password_file")
        if [ -n "$pw_file" ]; then
            echo "${pw_file/#\~/$HOME}"
            return
        fi
    fi
}

# Get telegram notify config from defaults section
# Usage: get_default_telegram_config
get_default_telegram_config() {
    awk '
    BEGIN { in_defaults=0; in_notify=0; in_telegram=0; token=""; chat_id=""; thread_id="" }
    /^defaults:/ { in_defaults=1; next }
    /^[a-zA-Z]/ { if ($0 !~ /^defaults:/) { in_defaults=0; in_notify=0; in_telegram=0 } }
    in_defaults && /^  notify:/ { in_notify=1; next }
    in_defaults && /^  [a-zA-Z]/ { if ($0 !~ /^  notify:/) { in_notify=0; in_telegram=0 } }
    in_notify && /^    telegram:/ { in_telegram=1; next }
    in_notify && /^    [a-zA-Z]/ { if ($0 !~ /^    telegram:/) in_telegram=0 }
    in_telegram && /^      token:/ {
        val=$0; gsub(/^[^:]+: */, "", val); gsub(/^["'\''"]|["'\''"]$/, "", val)
        token=val
    }
    in_telegram && /^      chat_id:/ {
        val=$0; gsub(/^[^:]+: */, "", val); gsub(/^["'\''"]|["'\''"]$/, "", val)
        chat_id=val
    }
    in_telegram && /^      message_thread_id:/ {
        val=$0; gsub(/^[^:]+: */, "", val); gsub(/^["'\''"]|["'\''"]$/, "", val)
        thread_id=val
    }
    END {
        if (token != "" && chat_id != "") {
            printf "telegram:%s:%s", token, chat_id
            if (thread_id != "") printf ":%s", thread_id
            print ""
        }
    }
    ' "$JOBS_FILE"
}

# Get telegram notify config from job (3-level nested: notify.telegram.*)
# Falls back to defaults if job doesn't have notify config
# Usage: get_job_telegram_config "job_name"
get_job_telegram_config() {
    local job="$1"

    local config=$(awk -v job="$job" '
    BEGIN { in_job=0; in_notify=0; in_telegram=0; token=""; chat_id=""; thread_id="" }
    /^  [a-zA-Z0-9_-]+:/ {
        gsub(/:.*/, "", $1)
        if ($1 == job) { in_job=1 } else { in_job=0; in_notify=0; in_telegram=0 }
        next
    }
    in_job && /^    notify:/ { in_notify=1; next }
    in_job && /^    [a-zA-Z]/ { if ($0 !~ /^    notify:/) { in_notify=0; in_telegram=0 } }
    in_notify && /^      telegram:/ { in_telegram=1; next }
    in_notify && /^      [a-zA-Z]/ { if ($0 !~ /^      telegram:/) in_telegram=0 }
    in_telegram && /^        token:/ {
        val=$0; gsub(/^[^:]+: */, "", val); gsub(/^["'\''"]|["'\''"]$/, "", val)
        token=val
    }
    in_telegram && /^        chat_id:/ {
        val=$0; gsub(/^[^:]+: */, "", val); gsub(/^["'\''"]|["'\''"]$/, "", val)
        chat_id=val
    }
    in_telegram && /^        message_thread_id:/ {
        val=$0; gsub(/^[^:]+: */, "", val); gsub(/^["'\''"]|["'\''"]$/, "", val)
        thread_id=val
    }
    END {
        if (token != "" && chat_id != "") {
            printf "telegram:%s:%s", token, chat_id
            if (thread_id != "") printf ":%s", thread_id
            print ""
        }
    }
    ' "$JOBS_FILE")

    # Fallback to defaults if job doesn't have notify
    if [ -z "$config" ]; then
        config=$(get_default_telegram_config)
    fi

    echo "$config"
}

# Parse job and build command arguments
# Usage: parse_job_to_args "job_name"
parse_job_to_args() {
    local job="$1"
    local args=""

    # Check if job has old-style args
    local old_args=$(get_job_field "$job" "args")
    if [ -n "$old_args" ]; then
        echo "$old_args"
        return
    fi

    # Build from new format
    local from_conn=$(build_connection "$job" "from")
    local to_conn=$(build_connection "$job" "to")

    [ -n "$from_conn" ] && args+=" --from '$from_conn'"
    [ -n "$to_conn" ] && args+=" --to '$to_conn'"

    # Password files
    local from_pw=$(get_password_file "$job" "from")
    local to_pw=$(get_password_file "$job" "to")

    [ -n "$from_pw" ] && args+=" --from-password-file '$from_pw'"
    [ -n "$to_pw" ] && args+=" --to-password-file '$to_pw'"

    # Options (with defaults fallback)
    # Boolean flags
    [ "$(get_job_value "$job" "force")" = "true" ] && args+=" --force"
    [ "$(get_job_value "$job" "verbose")" = "true" ] && args+=" --verbose"
    [ "$(get_job_value "$job" "quiet")" = "true" ] && args+=" --quiet"
    [ "$(get_job_value "$job" "dry_run")" = "true" ] && args+=" --dry-run"
    [ "$(get_job_value "$job" "no_meta")" = "true" ] && args+=" --no-meta"
    [ "$(get_job_value "$job" "sudo")" = "true" ] && args+=" --sudo"
    [ "$(get_job_value "$job" "stream")" = "true" ] && args+=" --stream"
    [ "$(get_job_value "$job" "notify_on_error")" = "true" ] && args+=" --notify-on-error"

    # Storage options
    [ -n "$(get_job_value "$job" "output")" ] && args+=" --output '$(get_job_value "$job" "output")'"
    [ -n "$(get_job_value "$job" "dump_name")" ] && args+=" --dump-name '$(get_job_value "$job" "dump_name")'"
    [ -n "$(get_job_value "$job" "keep")" ] && args+=" --keep $(get_job_value "$job" "keep")"
    [ -n "$(get_job_value "$job" "from_keep")" ] && args+=" --from-keep $(get_job_value "$job" "from_keep")"
    [ -n "$(get_job_value "$job" "skip_if_recent")" ] && args+=" --skip-if-recent '$(get_job_value "$job" "skip_if_recent")'"

    # Filtering options
    [ -n "$(get_job_value "$job" "exclude_table")" ] && args+=" --exclude-table '$(get_job_value "$job" "exclude_table")'"
    [ -n "$(get_job_value "$job" "exclude_data")" ] && args+=" --exclude-data '$(get_job_value "$job" "exclude_data")'"
    [ -n "$(get_job_value "$job" "exclude_schema")" ] && args+=" --exclude-schema '$(get_job_value "$job" "exclude_schema")'"
    [ -n "$(get_job_value "$job" "only_table")" ] && args+=" --only-table '$(get_job_value "$job" "only_table")'"
    [ -n "$(get_job_value "$job" "only_schema")" ] && args+=" --only-schema '$(get_job_value "$job" "only_schema")'"

    # Compression options
    [ -n "$(get_job_value "$job" "compress")" ] && args+=" --compress '$(get_job_value "$job" "compress")'"
    [ -n "$(get_job_value "$job" "compress_level")" ] && args+=" --compress-level $(get_job_value "$job" "compress_level")"
    [ -n "$(get_job_value "$job" "pg_compress_level")" ] && args+=" --pg-compress-level $(get_job_value "$job" "pg_compress_level")"

    # Streaming options
    [ -n "$(get_job_value "$job" "stream_buffer")" ] && args+=" --stream-buffer $(get_job_value "$job" "stream_buffer")"

    # Health check options
    [ "$(get_job_value "$job" "health_check")" = "true" ] && args+=" --health-check"
    [ "$(get_job_value "$job" "health_check_after")" = "true" ] && args+=" --health-check-after"
    [ "$(get_job_value "$job" "no_health_check")" = "true" ] && args+=" --no-health-check"
    [ "$(get_job_value "$job" "health_check_fail")" = "true" ] && args+=" --health-check-fail"

    # Retention (GFS) options
    [ "$(get_job_value "$job" "retention")" = "true" ] && args+=" --retention"
    [ -n "$(get_job_value "$job" "retention_daily")" ] && args+=" --retention-daily $(get_job_value "$job" "retention_daily")"
    [ -n "$(get_job_value "$job" "retention_weekly")" ] && args+=" --retention-weekly $(get_job_value "$job" "retention_weekly")"
    [ -n "$(get_job_value "$job" "retention_monthly")" ] && args+=" --retention-monthly $(get_job_value "$job" "retention_monthly")"
    [ -n "$(get_job_value "$job" "retention_yearly")" ] && args+=" --retention-yearly $(get_job_value "$job" "retention_yearly")"

    # Masking options
    [ "$(get_job_value "$job" "mask")" = "true" ] && args+=" --mask"
    [ -n "$(get_job_value "$job" "mask_rules")" ] && args+=" --mask-rules '$(get_job_value "$job" "mask_rules")'"
    [ -n "$(get_job_value "$job" "mask_tables")" ] && args+=" --mask-tables '$(get_job_value "$job" "mask_tables")'"

    # Logging options
    [ -n "$(get_job_value "$job" "log")" ] && args+=" --log '$(get_job_value "$job" "log")'"
    [ -n "$(get_job_value "$job" "log_level")" ] && args+=" --log-level '$(get_job_value "$job" "log_level")'"

    # Notify - telegram
    local telegram_config=$(get_job_telegram_config "$job")
    [ -n "$telegram_config" ] && args+=" --notify '$telegram_config'"

    echo "$args"
}

save_job() {
    local job_name="$1"
    shift
    local job_command="$1"

    [ -z "$job_name" ] && { log_error "Job name required"; return 1; }
    [ -z "$job_command" ] && { log_error "Command required"; return 1; }

    # Build args string from current settings (use single quotes for values with special chars)
    local args=""
    [ -n "$FROM_CONNECTION" ] && args="$args --from '$FROM_CONNECTION'"
    for to in "${TO_CONNECTIONS[@]}"; do
        args="$args --to '$to'"
    done
    [ -n "$FROM_PASSWORD_FILE" ] && args="$args --from-password-file '$FROM_PASSWORD_FILE'"
    for pf in "${TO_PASSWORD_FILES[@]}"; do
        args="$args --to-password-file '$pf'"
    done
    [ -n "$PASSWORD_FILE" ] && args="$args --password-file '$PASSWORD_FILE'"
    [ -n "$OUTPUT_DIR" ] && args="$args --output '$OUTPUT_DIR'"
    [ "$KEEP" -ge 0 ] && args="$args --keep $KEEP"
    [ "$FROM_KEEP" -gt 0 ] && args="$args --from-keep $FROM_KEEP"
    [ -n "$EXCLUDE_TABLES" ] && args="$args --exclude-table '$EXCLUDE_TABLES'"
    [ -n "$EXCLUDE_SCHEMAS" ] && args="$args --exclude-schema '$EXCLUDE_SCHEMAS'"
    [ -n "$ONLY_TABLES" ] && args="$args --only-table '$ONLY_TABLES'"
    [ -n "$ONLY_SCHEMAS" ] && args="$args --only-schema '$ONLY_SCHEMAS'"
    [ "$FORCE" = true ] && args="$args --force"
    [ "$VERBOSE" = true ] && args="$args --verbose"

    # Create jobs file if not exists
    [ ! -f "$JOBS_FILE" ] && echo "jobs:" > "$JOBS_FILE"

    # Check if job exists
    if grep -q "^  $job_name:" "$JOBS_FILE" 2>/dev/null; then
        # Update existing job
        local tmp_file=$(mktemp)
        awk -v name="$job_name" -v cmd="$job_command" -v args="$args" '
        BEGIN { skip=0 }
        /^  [a-zA-Z0-9_-]+:/ {
            if ($1 == name":") {
                skip=1
                print "  " name ":"
                print "    command: " cmd
                print "    args: \"" args "\""
                next
            } else {
                skip=0
            }
        }
        skip==0 { print }
        skip==1 && /^  [a-zA-Z0-9_-]+:/ { skip=0; print }
        ' "$JOBS_FILE" > "$tmp_file"
        mv "$tmp_file" "$JOBS_FILE"
        log_success "Updated job: $job_name"
    else
        # Add new job
        cat >> "$JOBS_FILE" << EOF
  $job_name:
    command: $job_command
    args:$args
EOF
        log_success "Saved job: $job_name"
    fi

    log_info "Jobs file: $JOBS_FILE"
}

list_jobs() {
    if [ ! -f "$JOBS_FILE" ]; then
        log_warn "No jobs file found: $JOBS_FILE"
        return 1
    fi

    echo ""
    echo "Available jobs:"
    echo "==============="
    # Only list jobs under the 'jobs:' section
    awk '
    /^jobs:/ { in_jobs=1; next }
    /^[a-zA-Z]/ { if ($0 !~ /^jobs:/) in_jobs=0 }
    in_jobs && /^  [a-zA-Z0-9_-]+:/ {
        gsub(/^ +/, "")
        gsub(/:.*/, "")
        print
    }
    ' "$JOBS_FILE" | while read -r job; do
        if [[ "$job" == *"-to-local"* ]]; then
            echo -e "  ‚Ä¢ ${MAGENTA}${job}${NC}"
        elif [[ "$job" == *"-to-30"* ]]; then
            echo -e "  ‚Ä¢ ${CYAN}${job}${NC}"
        else
            echo "  ‚Ä¢ $job"
        fi
    done
    echo ""
    echo "Usage: t-pgsql jobs [list|show|remove] <name>"
    echo ""
}

show_job() {
    local job_name="$1"

    if [ -z "$job_name" ]; then
        log_error "Job name required. Usage: t-pgsql jobs show <name>"
        return 1
    fi

    if [ ! -f "$JOBS_FILE" ]; then
        log_error "Jobs file not found: $JOBS_FILE"
        return 1
    fi

    # Check if job exists
    local job_cmd=$(get_job_field "$job_name" "command")
    if [ -z "$job_cmd" ]; then
        log_error "Job not found: $job_name"
        return 1
    fi

    # Parse job args (supports both old and new format)
    local job_args=$(parse_job_to_args "$job_name")

    echo ""
    echo -e "Job: ${BOLD}$job_name${NC}"
    echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
    echo -e "${CYAN}Command:${NC} $job_cmd"
    echo -e "${CYAN}Arguments:${NC}"

    # Parse and display args nicely (handle quoted values)
    echo "$job_args" | sed "s/' --/'\n--/g" | while read -r arg; do
        [ -n "$arg" ] && echo -e "  ${GREEN}${arg}${NC}"
    done
    echo ""
}

remove_job() {
    local job_name="$1"

    if [ -z "$job_name" ]; then
        log_error "Job name required. Usage: t-pgsql jobs remove <name>"
        return 1
    fi

    if [ ! -f "$JOBS_FILE" ]; then
        log_error "Jobs file not found: $JOBS_FILE"
        return 1
    fi

    # Check if job exists
    if ! grep -q "^  $job_name:" "$JOBS_FILE" 2>/dev/null; then
        log_error "Job not found: $job_name"
        return 1
    fi

    # Remove job using awk
    local tmp_file=$(mktemp)
    awk -v name="$job_name" '
    BEGIN { skip=0 }
    /^  [a-zA-Z0-9_-]+:/ {
        if ($1 == name":") {
            skip=1
            next
        } else {
            skip=0
        }
    }
    skip==0 { print }
    ' "$JOBS_FILE" > "$tmp_file"

    mv "$tmp_file" "$JOBS_FILE"
    log_success "Removed job: $job_name"
}

run_job() {
    local job_name="$1"

    if [ ! -f "$JOBS_FILE" ]; then
        log_error "Jobs file not found: $JOBS_FILE"
        return 1
    fi

    # Extract command
    local job_cmd=$(get_job_field "$job_name" "command")

    if [ -z "$job_cmd" ]; then
        log_error "Job not found: $job_name"
        return 1
    fi

    # Parse job args (supports both old and new format)
    local job_args=$(parse_job_to_args "$job_name")

    log_info "Running job: $job_name"
    log_debug "Command: $job_cmd $job_args"

    # Execute the job using bash -c to properly handle quotes
    bash -c "'$0' $job_cmd $job_args"
}

cmd_batch() {
    local target="$1"

    # Load defaults from jobs.yaml (parallel, continue_on_error)
    load_batch_defaults

    if [ -z "$target" ]; then
        if [ ! -f "$JOBS_FILE" ]; then
            log_error "Jobs file not found: $JOBS_FILE"
            return 1
        fi

        # Get jobs list
        local jobs_array=()
        while IFS= read -r job; do
            jobs_array+=("$job")
        done < <(awk '
            /^jobs:/ { in_jobs=1; next }
            /^[a-zA-Z]/ { if ($0 !~ /^jobs:/) in_jobs=0 }
            in_jobs && /^  [a-zA-Z0-9_-]+:/ {
                gsub(/^ +/, "")
                gsub(/:.*/, "")
                print
            }
        ' "$JOBS_FILE")

        if [ ${#jobs_array[@]} -eq 0 ]; then
            log_warn "No jobs found"
            return 1
        fi

        echo ""
        echo "Available jobs:"
        echo "==============="
        local i=1
        for job in "${jobs_array[@]}"; do
            if [[ "$job" == *"-to-local"* ]]; then
                echo -e "  ${BOLD}$i)${NC} ${MAGENTA}${job}${NC}"
            elif [[ "$job" == *"-to-30"* ]]; then
                echo -e "  ${BOLD}$i)${NC} ${CYAN}${job}${NC}"
            else
                echo -e "  ${BOLD}$i)${NC} $job"
            fi
            i=$((i + 1))
        done
        echo ""

        # Ask for selection
        read -p "Select job (1-${#jobs_array[@]}): " selection

        # Validate selection
        if ! [[ "$selection" =~ ^[0-9]+$ ]] || [ "$selection" -lt 1 ] || [ "$selection" -gt ${#jobs_array[@]} ]; then
            log_error "Invalid selection"
            return 1
        fi

        target="${jobs_array[$((selection - 1))]}"

        # Confirmation
        echo ""
        if [[ "$target" == *"-to-local"* ]]; then
            echo -e "Selected: ${MAGENTA}${target}${NC}"
        elif [[ "$target" == *"-to-30"* ]]; then
            echo -e "Selected: ${CYAN}${target}${NC}"
        else
            echo "Selected: $target"
        fi
        echo ""
        read -p "Are you sure? (y/N): " confirm
        if [[ ! "$confirm" =~ ^[Yy]$ ]]; then
            log_warn "Cancelled"
            return 0
        fi
        echo ""
    fi

    if [ "$target" = "all" ]; then
        if [ ! -f "$JOBS_FILE" ]; then
            log_error "Jobs file not found: $JOBS_FILE"
            return 1
        fi

        meta_start
        log_info "Running all jobs..."
        echo ""

        local all_jobs=$(awk '
            /^jobs:/ { in_jobs=1; next }
            /^[a-zA-Z]/ { if ($0 !~ /^jobs:/) in_jobs=0 }
            in_jobs && /^  [a-zA-Z0-9_-]+:/ {
                gsub(/^ +/, "")
                gsub(/:.*/, "")
                print
            }
        ' "$JOBS_FILE")

        # Filter jobs with --only and --exclude
        local jobs=""
        for job in $all_jobs; do
            local include=true

            # Check --only filter
            if [ -n "$ONLY_JOBS" ]; then
                include=false
                IFS=',' read -ra only_arr <<< "$ONLY_JOBS"
                for o in "${only_arr[@]}"; do
                    [[ "$job" == $(echo "$o" | xargs) ]] && include=true
                done
            fi

            # Check --exclude filter
            if [ -n "$EXCLUDE_JOBS" ] && [ "$include" = true ]; then
                IFS=',' read -ra excl_arr <<< "$EXCLUDE_JOBS"
                for e in "${excl_arr[@]}"; do
                    [[ "$job" == $(echo "$e" | xargs) ]] && include=false
                done
            fi

            [ "$include" = true ] && jobs="$jobs $job"
        done
        jobs=$(echo "$jobs" | xargs)

        if [ -z "$jobs" ]; then
            log_warn "No jobs to run (check --only/--exclude filters)"
            return 0
        fi

        local total=$(echo "$jobs" | wc -w | tr -d ' ')
        local current=0
        local failed=0

        if [ "$PARALLEL" -gt 1 ]; then
            # Parallel execution
            log_info "Running $total jobs with $PARALLEL parallel workers..."
            echo ""

            local pids=()
            local job_names=()
            local running=0
            local completed=0
            local temp_dir=$(mktemp -d)

            for job in $jobs; do
                # Wait if we've reached max parallel jobs
                while [ $running -ge $PARALLEL ]; do
                    for i in "${!pids[@]}"; do
                        if ! kill -0 "${pids[$i]}" 2>/dev/null; then
                            wait "${pids[$i]}" 2>/dev/null
                            local exit_code=$?
                            completed=$((completed + 1))

                            if [ $exit_code -eq 0 ]; then
                                log_success "[${completed}/${total}] Completed: ${job_names[$i]}"
                            else
                                log_error "[${completed}/${total}] Failed: ${job_names[$i]}"
                                failed=$((failed + 1))
                            fi

                            unset pids[$i]
                            unset job_names[$i]
                            running=$((running - 1))
                        fi
                    done
                    sleep 0.5
                done

                # Start new job in background
                log_info "Starting: $job"
                (run_job "$job" > "$temp_dir/$job.log" 2>&1; echo $? > "$temp_dir/$job.exit") &
                pids+=($!)
                job_names+=("$job")
                running=$((running + 1))
            done

            # Wait for remaining jobs
            for i in "${!pids[@]}"; do
                wait "${pids[$i]}" 2>/dev/null
                local exit_code=$(cat "$temp_dir/${job_names[$i]}.exit" 2>/dev/null || echo "1")
                completed=$((completed + 1))

                if [ "$exit_code" -eq 0 ]; then
                    log_success "[${completed}/${total}] Completed: ${job_names[$i]}"
                else
                    log_error "[${completed}/${total}] Failed: ${job_names[$i]}"
                    failed=$((failed + 1))
                fi
            done

            rm -rf "$temp_dir"
        else
            # Sequential execution
            for job in $jobs; do
                current=$((current + 1))
                echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
                log_info "[$current/$total] Job: $job"
                echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"

                if run_job "$job"; then
                    log_success "[$current/$total] Completed: $job"
                else
                    log_error "[$current/$total] Failed: $job"
                    failed=$((failed + 1))
                    [ "$CONTINUE_ON_ERROR" != true ] && { log_error "Stopping (use --continue-on-error to continue)"; return 1; }
                fi
                echo ""
            done
        fi

        echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
        if [ $failed -eq 0 ]; then
            log_success "All $total jobs completed successfully"
        else
            log_warn "$failed of $total jobs failed"
        fi

        # Send summary notification
        if [ "$NOTIFY_SUMMARY" = true ] && [ ${#NOTIFY[@]} -gt 0 ]; then
            local status="success"
            [ $failed -gt 0 ] && status="failed"
            local elapsed=$(format_elapsed $(($(date +%s) - META_START_EPOCH)))
            local summary="Completed: $((total - failed))/$total\nFailed: $failed\nDuration: $elapsed"
            send_notification "$status" "Batch completed: $total jobs" "$summary"
        fi
    else
        run_job "$target"
    fi
}

# ==============================================================================
# UTILITIES
# ==============================================================================
compress_file() {
    local file="$1"
    log_info "Compressing ($COMPRESS)..."

    case "$COMPRESS" in
        zstd)  zstd -${COMPRESS_LEVEL} --rm "$file" ;;
        xz)    xz -${COMPRESS_LEVEL} "$file" ;;
        bzip2) bzip2 -${COMPRESS_LEVEL} "$file" ;;
    esac

    [ $? -eq 0 ] && log_success "Compressed" || log_error "Compression failed"
}

cleanup_old_dumps() {
    [ -z "$FROM_DATABASE" ] && return 0

    # Use GFS retention if enabled
    if [ "$RETENTION" = true ]; then
        cleanup_gfs
        return 0
    fi

    # Simple keep-N retention
    [ "$KEEP" -le 0 ] && return 0

    local dump_base_name="${DUMP_NAME:-$FROM_DATABASE}"
    local count=$(find "$OUTPUT_DIR" -name "${dump_base_name}_*.tar.gz" -type f 2>/dev/null | wc -l)

    if [ "$count" -gt "$KEEP" ]; then
        local del=$((count - KEEP))
        log_info "Deleting $del old dump(s)..."

        # Use ls for macOS compatibility
        ls -t "$OUTPUT_DIR"/${dump_base_name}_*.tar.gz 2>/dev/null | tail -n "$del" | \
            while read f; do
                rm -f "$f"
                log_debug "Deleted: $f"
            done
    fi
}

# GFS (Grandfather-Father-Son) Retention Policy
cleanup_gfs() {
    log_info "Applying GFS retention policy..."
    log_debug "Daily: $RETENTION_DAILY, Weekly: $RETENTION_WEEKLY, Monthly: $RETENTION_MONTHLY, Yearly: $RETENTION_YEARLY"

    local dump_base_name="${DUMP_NAME:-$FROM_DATABASE}"
    local keep_files=""
    local all_files=$(ls -t "$OUTPUT_DIR"/${dump_base_name}_*.tar.gz 2>/dev/null)

    [ -z "$all_files" ] && return 0

    local today=$(date +%Y%m%d)
    local daily_kept=0
    local weekly_kept=0
    local monthly_kept=0
    local yearly_kept=0

    # Process each file
    while read -r file; do
        [ -z "$file" ] && continue

        local fname=$(basename "$file")
        # Extract date from filename: database_YYYYMMDD_HHMMSS.tar.gz
        local file_date=$(echo "$fname" | sed -n 's/.*_\([0-9]\{8\}\)_[0-9]\{6\}\.tar\.gz/\1/p')

        [ -z "$file_date" ] && continue

        local keep=false
        local reason=""

        # Calculate days ago
        local file_epoch=$(date -j -f "%Y%m%d" "$file_date" "+%s" 2>/dev/null || date -d "$file_date" "+%s" 2>/dev/null)
        local today_epoch=$(date "+%s")
        local days_ago=$(( (today_epoch - file_epoch) / 86400 ))

        # Daily: keep last N days
        if [ $daily_kept -lt $RETENTION_DAILY ] && [ $days_ago -lt $RETENTION_DAILY ]; then
            keep=true
            reason="daily"
            daily_kept=$((daily_kept + 1))
        fi

        # Weekly: keep if it's a Sunday (day 0) or last file of the week
        local day_of_week=$(date -j -f "%Y%m%d" "$file_date" "+%u" 2>/dev/null || date -d "$file_date" "+%u" 2>/dev/null)
        if [ "$day_of_week" = "7" ] && [ $weekly_kept -lt $RETENTION_WEEKLY ]; then
            keep=true
            reason="weekly"
            weekly_kept=$((weekly_kept + 1))
        fi

        # Monthly: keep if it's last day of month or first file of the month
        local day_of_month=$(echo "$file_date" | cut -c7-8)
        local year_month=$(echo "$file_date" | cut -c1-6)
        local next_month_first=$(date -j -f "%Y%m%d" "${file_date}" "+%Y%m01" 2>/dev/null | xargs -I{} date -j -v+1m -f "%Y%m%d" {} "+%Y%m%d" 2>/dev/null)
        local last_day_of_month=$(date -j -v-1d -f "%Y%m%d" "$next_month_first" "+%d" 2>/dev/null || echo "28")

        if [ "$day_of_month" = "$last_day_of_month" ] && [ $monthly_kept -lt $RETENTION_MONTHLY ]; then
            keep=true
            reason="monthly"
            monthly_kept=$((monthly_kept + 1))
        fi

        # Yearly: keep if it's Dec 31
        local month_day=$(echo "$file_date" | cut -c5-8)
        if [ "$month_day" = "1231" ] && [ $yearly_kept -lt $RETENTION_YEARLY ]; then
            keep=true
            reason="yearly"
            yearly_kept=$((yearly_kept + 1))
        fi

        if [ "$keep" = true ]; then
            keep_files="$keep_files $file"
            log_debug "Keep ($reason): $fname"
        fi
    done <<< "$all_files"

    # Delete files not in keep list
    local deleted=0
    while read -r file; do
        [ -z "$file" ] && continue
        if ! echo "$keep_files" | grep -q "$file"; then
            rm -f "$file"
            log_debug "Deleted: $(basename "$file")"
            deleted=$((deleted + 1))
        fi
    done <<< "$all_files"

    [ $deleted -gt 0 ] && log_info "GFS cleanup: deleted $deleted file(s)"
    log_debug "Kept: daily=$daily_kept, weekly=$weekly_kept, monthly=$monthly_kept, yearly=$yearly_kept"
}

# ==============================================================================
# ARGUMENT PARSER
# ==============================================================================
parse_args() {
    [ $# -eq 0 ] && { show_help; exit 0; }

    # Check if first arg is option or command
    case "$1" in
        -h|--help) show_help; exit 0 ;;
        --version) show_version; exit 0 ;;
        --batch) BATCH_JOB="$2"; shift 2 ;;
        -*) log_error "Command required. Use --help for usage."; exit 1 ;;
        *) COMMAND="$1"; shift ;;
    esac

    # Handle jobs subcommand (jobs list|show|remove <name>)
    if [ "$COMMAND" = "jobs" ] && [ $# -gt 0 ]; then
        case "$1" in
            list|show|remove)
                JOBS_ACTION="$1"
                shift
                # Skip options and get target
                while [[ $# -gt 0 ]]; do
                    if [[ "$1" == -* ]]; then
                        # It's an option, will be parsed in main loop
                        break
                    else
                        JOBS_TARGET="$1"
                        shift
                        break
                    fi
                done
                ;;
            -*)
                # It's an option, not a subcommand
                ;;
            *)
                # Assume it's a job name for 'show' as default action
                JOBS_ACTION="show"
                JOBS_TARGET="$1"
                shift
                ;;
        esac
    fi

    # Handle batch subcommand (batch <job|all>)
    if [ "$COMMAND" = "batch" ] && [ $# -gt 0 ] && [[ "$1" != -* ]]; then
        BATCH_JOB="$1"
        shift
    fi

    while [[ $# -gt 0 ]]; do
        case $1 in
            --from) FROM_CONNECTION="$2"; shift 2 ;;
            --to) TO_CONNECTIONS+=("$2"); shift 2 ;;

            --password) PASSWORD="$2"; shift 2 ;;
            --from-password) FROM_PASSWORD="$2"; shift 2 ;;
            --to-password) TO_PASSWORD="$2"; shift 2 ;;
            --password-file) PASSWORD_FILE="$2"; shift 2 ;;
            --from-password-file) FROM_PASSWORD_FILE="$2"; shift 2 ;;
            --to-password-file) TO_PASSWORD_FILES+=("$2"); shift 2 ;;
            --config) CONFIG_FILE="$2"; shift 2 ;;

            --exclude-table) EXCLUDE_TABLES="$2"; shift 2 ;;
            --exclude-schema) EXCLUDE_SCHEMAS="$2"; shift 2 ;;
            --exclude-data) EXCLUDE_DATA="$2"; shift 2 ;;
            --only-table) ONLY_TABLES="$2"; shift 2 ;;
            --only-schema) ONLY_SCHEMAS="$2"; shift 2 ;;

            --compress) COMPRESS="$2"; shift 2 ;;
            --compress-level) COMPRESS_LEVEL="$2"; shift 2 ;;
            --pg-compress-level) PG_COMPRESS_LEVEL="$2"; shift 2 ;;

            --output) OUTPUT_DIR="$2"; shift 2 ;;
            --keep) KEEP="$2"; shift 2 ;;
            --from-keep) FROM_KEEP="$2"; shift 2 ;;
            --dump-name) DUMP_NAME="$2"; shift 2 ;;
            --skip-if-recent) SKIP_IF_RECENT="$2"; shift 2 ;;
            --from-file)
                # Optional value: --from-file or --from-file <pattern>
                if [[ -n "$2" && ! "$2" =~ ^-- ]]; then
                    FROM_FILE="$2"
                    shift 2
                else
                    FROM_FILE="latest"
                    shift
                fi
                ;;

            --retention) RETENTION=true; shift ;;
            --retention-daily) RETENTION_DAILY="$2"; shift 2 ;;
            --retention-weekly) RETENTION_WEEKLY="$2"; shift 2 ;;
            --retention-monthly) RETENTION_MONTHLY="$2"; shift 2 ;;
            --retention-yearly) RETENTION_YEARLY="$2"; shift 2 ;;

            --health-check) HEALTH_CHECK=true; shift ;;
            --health-check-after) HEALTH_CHECK_AFTER=true; shift ;;
            --no-health-check) HEALTH_CHECK=false; shift ;;
            --health-check-fail) HEALTH_CHECK_FAIL=true; shift ;;

            --notify) NOTIFY+=("$2"); shift 2 ;;
            --notify-on-error) NOTIFY_ON_ERROR=true; shift ;;

            --mask) MASK=true; shift ;;
            --mask-rules) MASK_RULES="$2"; shift 2 ;;
            --mask-tables) MASK_TABLES="$2"; shift 2 ;;

            --stream) STREAM=true; shift ;;
            --stream-buffer) STREAM_BUFFER="$2"; shift 2 ;;

            --sudo) SUDO=true; shift ;;

            --parallel) PARALLEL="$2"; shift 2 ;;
            --continue-on-error) CONTINUE_ON_ERROR=true; shift ;;
            --only) ONLY_JOBS="$2"; shift 2 ;;
            --exclude) EXCLUDE_JOBS="$2"; shift 2 ;;
            --notify-summary) NOTIFY_SUMMARY=true; shift ;;
            --save) SAVE_JOB="$2"; shift 2 ;;
            --batch) BATCH_JOB="$2"; shift 2 ;;
            --yaml)
                if [[ "$2" == *".yaml" ]]; then
                    # Already has .yaml extension
                    JOBS_FILE="$2"
                elif [[ "$2" == *"/"* ]]; then
                    # Contains path but no .yaml extension
                    JOBS_FILE="${2}.yaml"
                else
                    # Just a name, use SCRIPT_DIR
                    JOBS_FILE="${SCRIPT_DIR}/${2}.yaml"
                fi
                shift 2 ;;

            --file) FILE="$2"; shift 2 ;;

            --log) LOG_FILE="$2"; shift 2 ;;
            --log-level) LOG_LEVEL="$2"; shift 2 ;;
            -v|--verbose) VERBOSE=true; shift ;;
            -q|--quiet) QUIET=true; shift ;;
            -y|--yes) YES=true; shift ;;
            -f|--force) FORCE=true; shift ;;
            --dry-run) DRY_RUN=true; shift ;;
            --no-meta) META_ENABLED=false; shift ;;
            -h|--help) show_help; exit 0 ;;
            --version) show_version; exit 0 ;;

            *) log_error "Unknown: $1"; exit 1 ;;
        esac
    done
}

# ==============================================================================
# MAIN
# ==============================================================================
main() {
    parse_args "$@"

    [ -n "$LOG_FILE" ] && mkdir -p "$(dirname "$LOG_FILE")"

    # Load config file if specified
    if [ -n "$CONFIG_FILE" ]; then
        load_config "$CONFIG_FILE" || exit 1
    fi

    # Handle --save: save current command as a job
    if [ -n "$SAVE_JOB" ]; then
        save_job "$SAVE_JOB" "$COMMAND"
        return 0
    fi

    # Handle batch with --batch argument (alternative to 'batch' command)
    if [ -n "$BATCH_JOB" ]; then
        cmd_batch "$BATCH_JOB"
        return 0
    fi

    case "$COMMAND" in
        dump)    cmd_dump ;;
        restore) cmd_restore ;;
        clone)   cmd_clone ;;
        fetch)   cmd_fetch ;;
        batch)   cmd_batch "$BATCH_JOB" ;;
        list)    cmd_list ;;
        meta)    cmd_meta ;;
        clean)   cmd_clean ;;
        jobs)
            case "$JOBS_ACTION" in
                show)   show_job "$JOBS_TARGET" ;;
                remove) remove_job "$JOBS_TARGET" ;;
                *)      list_jobs ;;
            esac
            ;;
        version) show_version ;;
        help)    show_help ;;
        *)       log_error "Unknown command: $COMMAND"; exit 1 ;;
    esac
}

main "$@"
